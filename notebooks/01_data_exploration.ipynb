{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba7ad41e",
   "metadata": {},
   "source": [
    "# Data Exploration - Amazon Beauty Products Recommendation System\n",
    "\n",
    "**B√†i to√°n:** X√¢y d·ª±ng h·ªá th·ªëng g·ª£i √Ω s·∫£n ph·∫©m l√†m ƒë·∫πp d·ª±a tr√™n l·ªãch s·ª≠ ƒë√°nh gi√° c·ªßa ng∆∞·ªùi d√πng\n",
    "\n",
    "**Dataset:** Amazon Ratings - Beauty Products\n",
    "\n",
    "## M·ª•c ti√™u:\n",
    "- Kh√°m ph√° v√† hi·ªÉu c·∫•u tr√∫c d·ªØ li·ªáu ratings\n",
    "- Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng (user behavior)\n",
    "- Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m (product characteristics)\n",
    "- Ph√¢n t√≠ch ph√¢n ph·ªëi ratings v√† patterns\n",
    "- ƒê·∫∑t c√¢u h·ªèi v√† tr·∫£ l·ªùi b·∫±ng d·ªØ li·ªáu\n",
    "- Ph√°t hi·ªán insights cho recommendation system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f125a",
   "metadata": {},
   "source": [
    "## 1. Import Libraries\n",
    "\n",
    "**Y√™u c·∫ßu:** CH·ªà s·ª≠ d·ª•ng NumPy ƒë·ªÉ x·ª≠ l√Ω d·ªØ li·ªáu, Matplotlib v√† Seaborn ƒë·ªÉ visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5446f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "# C·∫•u h√¨nh\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba2f56",
   "metadata": {},
   "source": [
    "## 2. Load Data using NumPy\n",
    "\n",
    "Load d·ªØ li·ªáu CSV ch·ªâ b·∫±ng NumPy (kh√¥ng d√πng Pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99678d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV data using NumPy\n",
    "data_path = '../data/raw/ratings_Beauty.csv'\n",
    "\n",
    "def load_csv_numpy(filepath, delimiter=',', skip_header=True):\n",
    "    \"\"\"\n",
    "    Load CSV file using only NumPy\n",
    "    Returns: data array, header, user_map, product_map\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # header\n",
    "    header = lines[0].strip().split(delimiter) if skip_header else None\n",
    "    \n",
    "    # data lines\n",
    "    data_lines = lines[1:] if skip_header else lines\n",
    "    \n",
    "    # Preallocate lists for each column\n",
    "    user_ids = []\n",
    "    product_ids = []\n",
    "    ratings = []\n",
    "    timestamps = []\n",
    "    \n",
    "    for line in data_lines:\n",
    "        parts = line.strip().split(delimiter)\n",
    "        user_ids.append(parts[0])\n",
    "        product_ids.append(parts[1])\n",
    "        ratings.append(float(parts[2]))\n",
    "        timestamps.append(int(parts[3]) if parts[3].isdigit() else 0)\n",
    "    \n",
    "    # convert lists to NumPy arrays\n",
    "    n_records = len(ratings)\n",
    "    data = np.zeros((n_records, 4))\n",
    "    \n",
    "    # user_ids and product_ids to indices\n",
    "    unique_users = list(set(user_ids))\n",
    "    unique_products = list(set(product_ids))\n",
    "    \n",
    "    user_id_map = {uid: idx for idx, uid in enumerate(unique_users)}\n",
    "    product_id_map = {pid: idx for idx, pid in enumerate(unique_products)}\n",
    "    \n",
    "    for i in range(n_records):\n",
    "        data[i, 0] = user_id_map[user_ids[i]]\n",
    "        data[i, 1] = product_id_map[product_ids[i]]\n",
    "        data[i, 2] = ratings[i]\n",
    "        data[i, 3] = timestamps[i]\n",
    "    \n",
    "    return data, header, user_id_map, product_id_map\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "data, header, user_map, product_map = load_csv_numpy(data_path)\n",
    "\n",
    "print(f\"‚úì Data loaded successfully!\")\n",
    "print(f\"  Shape: {data.shape}\")\n",
    "print(f\"  Columns: {header}\")\n",
    "print(f\"  Unique users: {len(user_map):,}\")\n",
    "print(f\"  Unique products: {len(product_map):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f193c2",
   "metadata": {},
   "source": [
    "## 3. Basic Data Information\n",
    "\n",
    "Hi·ªÉn th·ªã th√¥ng tin c∆° b·∫£n v·ªÅ dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "n_ratings = data.shape[0]\n",
    "n_users = len(np.unique(data[:, 0]))  # Column 0: UserId\n",
    "n_products = len(np.unique(data[:, 1]))  # Column 1: ProductId\n",
    "\n",
    "print(f\"Dataset Overview:\")\n",
    "print(f\"  Total ratings: {n_ratings:,}\")\n",
    "print(f\"  Unique users: {n_users:,}\")\n",
    "print(f\"  Unique products: {n_products:,}\")\n",
    "\n",
    "# Sample records\n",
    "print(f\"\\nSample Records (first 5):\")\n",
    "print(f\"{'UserId':<12} {'ProductId':<12} {'Rating':<10} {'Timestamp'}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(min(5, n_ratings)):\n",
    "    print(f\"{data[i, 0]:<12.0f} {data[i, 1]:<12.0f} {data[i, 2]:<10.1f} {data[i, 3]:<12.0f}\")\n",
    "\n",
    "# Sparsity metrics\n",
    "sparsity = (1 - n_ratings / (n_users * n_products)) * 100\n",
    "print(f\"\\nSparsity Metrics:\")\n",
    "print(f\"  Potential matrix size: {n_users:,} √ó {n_products:,}\")\n",
    "print(f\"  Sparsity: {sparsity:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe2b857",
   "metadata": {},
   "source": [
    "## 4. Descriptive Statistics\n",
    "\n",
    "T√≠nh to√°n th·ªëng k√™ m√¥ t·∫£ cho ratings s·ª≠ d·ª•ng NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a8a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ratings column\n",
    "ratings = data[:, 2]\n",
    "\n",
    "# Central tendency\n",
    "mean_rating = np.mean(ratings)\n",
    "median_rating = np.median(ratings)\n",
    "mode_rating = np.bincount(ratings.astype(int)).argmax()\n",
    "\n",
    "# Dispersion\n",
    "std_rating = np.std(ratings)\n",
    "var_rating = np.var(ratings)\n",
    "min_rating = np.min(ratings)\n",
    "max_rating = np.max(ratings)\n",
    "\n",
    "# Quantiles\n",
    "q25 = np.percentile(ratings, 25)\n",
    "q50 = np.percentile(ratings, 50)\n",
    "q75 = np.percentile(ratings, 75)\n",
    "iqr = q75 - q25\n",
    "\n",
    "# Shape metrics (Skewness and Kurtosis)\n",
    "centered = ratings - mean_rating\n",
    "skewness = np.mean(centered ** 3) / (std_rating ** 3)\n",
    "kurtosis = np.mean(centered ** 4) / (std_rating ** 4) - 3\n",
    "\n",
    "print(\"Descriptive Statistics for Ratings:\")\n",
    "print(f\"\\nCentral Tendency:\")\n",
    "print(f\"  Mean:   {mean_rating:.4f}\")\n",
    "print(f\"  Median: {median_rating:.4f}\")\n",
    "print(f\"  Mode:   {mode_rating:.0f}\")\n",
    "\n",
    "print(f\"\\nDispersion:\")\n",
    "print(f\"  Std Dev:  {std_rating:.4f}\")\n",
    "print(f\"  Variance: {var_rating:.4f}\")\n",
    "print(f\"  Range:    {max_rating - min_rating:.1f} (from {min_rating:.1f} to {max_rating:.1f})\")\n",
    "print(f\"  IQR:      {iqr:.4f}\")\n",
    "\n",
    "print(f\"\\nQuantiles:\")\n",
    "print(f\"  Q1 (25%):  {q25:.2f}\")\n",
    "print(f\"  Q2 (50%):  {q50:.2f}\")\n",
    "print(f\"  Q3 (75%):  {q75:.2f}\")\n",
    "\n",
    "print(f\"\\nDistribution Shape:\")\n",
    "print(f\"  Skewness: {skewness:.4f}\", end=\"\")\n",
    "if skewness < -0.5:\n",
    "    print(\" (left-skewed)\")\n",
    "elif skewness > 0.5:\n",
    "    print(\" (right-skewed)\")\n",
    "else:\n",
    "    print(\" (symmetric)\")\n",
    "\n",
    "print(f\"  Kurtosis: {kurtosis:.4f}\", end=\"\")\n",
    "if kurtosis > 0:\n",
    "    print(\" (heavy tails)\")\n",
    "elif kurtosis < 0:\n",
    "    print(\" (light tails)\")\n",
    "else:\n",
    "    print(\" (normal-like)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85935fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1 = axes[0]\n",
    "ax1.hist(ratings, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "ax1.axvline(mean_rating, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_rating:.2f}')\n",
    "ax1.axvline(median_rating, color='green', linestyle='--', linewidth=2, label=f'Median: {median_rating:.2f}')\n",
    "ax1.set_xlabel('Rating', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Ratings', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2 = axes[1]\n",
    "box = ax2.boxplot(ratings, vert=True, patch_artist=True, widths=0.5)\n",
    "box['boxes'][0].set_facecolor('lightblue')\n",
    "ax2.set_ylabel('Rating', fontsize=12)\n",
    "ax2.set_title('Box Plot of Ratings', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "stats_text = f\"Mean: {mean_rating:.2f}\\nMedian: {median_rating:.2f}\\nStd: {std_rating:.2f}\\nSkew: {skewness:.2f}\"\n",
    "ax2.text(1.3, max_rating * 0.5, stats_text, fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aada87d",
   "metadata": {},
   "source": [
    "## 5. Missing Values Analysis\n",
    "\n",
    "Ki·ªÉm tra missing values trong dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efc6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NaN values\n",
    "nan_counts = np.sum(np.isnan(data), axis=0)\n",
    "total_records = data.shape[0]\n",
    "\n",
    "column_names = ['UserId', 'ProductId', 'Rating', 'Timestamp']\n",
    "\n",
    "print(f\"\\nMissing Values Check:\")\n",
    "print(f\"{'Column':<15} {'Missing Count':<15} {'Percentage':<15} {'Status'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "has_missing = False\n",
    "for i, col_name in enumerate(column_names):\n",
    "    missing_count = int(nan_counts[i])\n",
    "    missing_pct = (missing_count / total_records) * 100\n",
    "    status = \"Clean\" if missing_count == 0 else \"Has Missing\"\n",
    "    print(f\"{col_name:<15} {missing_count:<15,} {missing_pct:<15.2f} {status}\")\n",
    "    if missing_count > 0:\n",
    "        has_missing = True\n",
    "\n",
    "if not has_missing:\n",
    "    print(\"No missing values detected\")\n",
    "else:\n",
    "    print(\"Missing values detected\")\n",
    "# zero values\n",
    "print(f\"\\nZero Values Check:\")\n",
    "zero_ratings = np.sum(data[:, 2] == 0)\n",
    "zero_timestamps = np.sum(data[:, 3] == 0)\n",
    "\n",
    "print(f\" Zero ratings: {zero_ratings:,} ({(zero_ratings/total_records)*100:.2f}%)\")\n",
    "print(f\" Zero timestamps: {zero_timestamps:,} ({(zero_timestamps/total_records)*100:.2f}%)\")\n",
    "\n",
    "if zero_ratings > 0:\n",
    "    print(\"   Warning: Some ratings are 0 (invalid for 1-5 scale)\")\n",
    "\n",
    "# Visualize missing data\n",
    "if has_missing or zero_ratings > 0 or zero_timestamps > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    issues = {\n",
    "        'UserId\\nNaN': nan_counts[0],\n",
    "        'ProductId\\nNaN': nan_counts[1],\n",
    "        'Rating\\nNaN': nan_counts[2],\n",
    "        'Timestamp\\nNaN': nan_counts[3],\n",
    "        'Rating\\nZero': zero_ratings,\n",
    "        'Timestamp\\nZero': zero_timestamps\n",
    "    }\n",
    "    \n",
    "    x_pos = np.arange(len(issues))\n",
    "    counts = list(issues.values())\n",
    "    colors = ['red' if c > 0 else 'green' for c in counts]\n",
    "    \n",
    "    bars = ax.bar(x_pos, counts, color=colors, alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Column / Issue Type', fontsize=12)\n",
    "    ax.set_ylabel('Count', fontsize=12)\n",
    "    ax.set_title('Data Quality Issues Overview', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(issues.keys(), rotation=45, ha='right')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # add value labels\n",
    "    for i, (bar, count) in enumerate(zip(bars, counts)):\n",
    "        if count > 0:\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "                   f'{count:,}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nCompleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9ba9a",
   "metadata": {},
   "source": [
    "## 6. Rating Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769d4a3",
   "metadata": {},
   "source": [
    "### 6.1. Rating Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fb9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count frequency of each rating value\n",
    "rating_values, rating_counts = np.unique(ratings, return_counts=True)\n",
    "total_ratings = len(ratings)\n",
    "\n",
    "# Calculate percentages\n",
    "rating_percentages = (rating_counts / total_ratings) * 100\n",
    "\n",
    "# Calculate statistics\n",
    "mean_rating = np.mean(ratings)\n",
    "median_rating = np.median(ratings)\n",
    "mode_idx = np.argmax(rating_counts)\n",
    "mode_rating = rating_values[mode_idx]\n",
    "\n",
    "print(\"Rating Frequency Statistics:\")\n",
    "print(f\"  Mean Rating: {mean_rating:.4f}\")\n",
    "print(f\"  Median Rating: {median_rating:.1f}\")\n",
    "print(f\"  Mode Rating: {mode_rating:.0f} (most frequent)\")\n",
    "print(f\"  Std Dev: {np.std(ratings):.4f}\")\n",
    "\n",
    "print(f\"\\nRating Distribution:\")\n",
    "print(f\"{'Rating':<10} {'Count':<15} {'Percentage'}\")\n",
    "print(\"-\" * 40)\n",
    "for val, count, pct in zip(rating_values, rating_counts, rating_percentages):\n",
    "    print(f\"{val:<10.1f} {count:<15,} {pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de45f519",
   "metadata": {},
   "source": [
    "### 6.2. Rating Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f129f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze rating bias\n",
    "positive_ratings = np.sum(ratings >= 4)  # 4 and 5 stars\n",
    "negative_ratings = np.sum(ratings <= 2)  # 1 and 2 stars\n",
    "neutral_ratings = np.sum(ratings == 3)   # 3 stars\n",
    "\n",
    "positive_pct = (positive_ratings / total_ratings) * 100\n",
    "negative_pct = (negative_ratings / total_ratings) * 100\n",
    "neutral_pct = (neutral_ratings / total_ratings) * 100\n",
    "\n",
    "print(\"Rating Sentiment Breakdown:\")\n",
    "print(f\"  Positive (4-5 stars): {positive_ratings:,} ({positive_pct:.2f}%)\")\n",
    "print(f\"  Neutral (3 stars): {neutral_ratings:,} ({neutral_pct:.2f}%)\")\n",
    "print(f\"  Negative (1-2 stars): {negative_ratings:,} ({negative_pct:.2f}%)\")\n",
    "\n",
    "if positive_pct > 60:\n",
    "    print(\"\\n  ‚ö† Positive bias detected - users tend to give high ratings\")\n",
    "elif negative_pct > 40:\n",
    "    print(\"\\n  ‚ö† Negative bias detected - users tend to give low ratings\")\n",
    "else:\n",
    "    print(\"\\n  ‚úì Ratings are relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98409a",
   "metadata": {},
   "source": [
    "### 6.3. Rating Distribution Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05dbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Rating Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Bar chart - Rating frequency\n",
    "ax1 = axes[0, 0]\n",
    "colors_bars = ['#d62728', '#ff7f0e', '#ffdd57', '#98df8a', '#2ca02c']\n",
    "bars = ax1.bar(rating_values, rating_counts, color=colors_bars, edgecolor='black', alpha=0.8, width=0.6)\n",
    "ax1.set_xlabel('Rating Value', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Rating Frequency Distribution', fontsize=13, fontweight='bold')\n",
    "ax1.set_xticks(rating_values)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, count, pct in zip(bars, rating_counts, rating_percentages):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(count):,}\\n({pct:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Pie chart - Rating proportions\n",
    "ax2 = axes[0, 1]\n",
    "explode = [0.05 if pct == max(rating_percentages) else 0 for pct in rating_percentages]\n",
    "ax2.pie(rating_counts, labels=[f'{int(r)}‚òÖ' for r in rating_values],\n",
    "        autopct='%1.1f%%', colors=colors_bars, explode=explode,\n",
    "        startangle=90, textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "ax2.set_title('Rating Proportions', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Histogram with density\n",
    "ax3 = axes[1, 0]\n",
    "ax3.hist(ratings, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.axvline(mean_rating, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_rating:.2f}')\n",
    "ax3.axvline(median_rating, color='green', linestyle='--', linewidth=2, label=f'Median: {median_rating:.1f}')\n",
    "ax3.set_xlabel('Rating Value', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Rating Histogram', fontsize=13, fontweight='bold')\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment categories bar chart\n",
    "ax4 = axes[1, 1]\n",
    "categories = ['Negative\\n(1-2‚òÖ)', 'Neutral\\n(3‚òÖ)', 'Positive\\n(4-5‚òÖ)']\n",
    "category_counts = np.array([negative_ratings, neutral_ratings, positive_ratings])\n",
    "category_colors = ['#e74c3c', '#f39c12', '#27ae60']\n",
    "\n",
    "bars4 = ax4.bar(categories, category_counts, color=category_colors, edgecolor='black', alpha=0.8)\n",
    "ax4.set_ylabel('Count', fontsize=12)\n",
    "ax4.set_title('Rating Sentiment Distribution', fontsize=13, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, count in zip(bars4, category_counts):\n",
    "    height = bar.get_height()\n",
    "    pct = (count / total_ratings) * 100\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{int(count):,}\\n({pct:.1f}%)',\n",
    "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef76b5a",
   "metadata": {},
   "source": [
    "## 7. User Behavior Analysis\n",
    "\n",
    "Ph√¢n t√≠ch h√†nh vi ng∆∞·ªùi d√πng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf604b9",
   "metadata": {},
   "source": [
    "### 7.1. User Activity Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353e2425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratings per user using NumPy\n",
    "user_ids = data[:, 0]\n",
    "unique_users, user_rating_counts = np.unique(user_ids, return_counts=True)\n",
    "\n",
    "n_users = len(unique_users)\n",
    "total_ratings = len(user_ids)\n",
    "\n",
    "# Statistics\n",
    "mean_ratings_per_user = np.mean(user_rating_counts)\n",
    "median_ratings_per_user = np.median(user_rating_counts)\n",
    "std_ratings_per_user = np.std(user_rating_counts)\n",
    "min_ratings_per_user = np.min(user_rating_counts)\n",
    "max_ratings_per_user = np.max(user_rating_counts)\n",
    "\n",
    "print(\"User Activity Statistics:\")\n",
    "print(f\"  Total unique users: {n_users:,}\")\n",
    "print(f\"  Mean ratings per user: {mean_ratings_per_user:.2f}\")\n",
    "print(f\"  Median ratings per user: {median_ratings_per_user:.1f}\")\n",
    "print(f\"  Std deviation: {std_ratings_per_user:.2f}\")\n",
    "print(f\"  Range: {min_ratings_per_user} to {max_ratings_per_user}\")\n",
    "\n",
    "# Top 10 most active users\n",
    "top_10_indices = np.argsort(user_rating_counts)[-10:][::-1]\n",
    "top_10_users = unique_users[top_10_indices]\n",
    "top_10_counts = user_rating_counts[top_10_indices]\n",
    "\n",
    "print(f\"\\nTop 10 Most Active Users:\")\n",
    "print(f\"{'Rank':<6} {'User ID':<15} {'# Ratings':<15} {'% of Total'}\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (user_id, count) in enumerate(zip(top_10_users, top_10_counts), 1):\n",
    "    pct = (count / total_ratings) * 100\n",
    "    print(f\"{rank:<6} {int(user_id):<15} {count:<15,} {pct:.4f}%\")\n",
    "\n",
    "# Cold start analysis\n",
    "users_with_le_5_ratings = np.sum(user_rating_counts <= 5)\n",
    "users_with_le_10_ratings = np.sum(user_rating_counts <= 10)\n",
    "\n",
    "pct_le_5 = (users_with_le_5_ratings / n_users) * 100\n",
    "pct_le_10 = (users_with_le_10_ratings / n_users) * 100\n",
    "\n",
    "print(f\"\\nCold Start Analysis:\")\n",
    "print(f\"  Users with ‚â§ 5 ratings: {users_with_le_5_ratings:,} ({pct_le_5:.2f}%)\")\n",
    "print(f\"  Users with ‚â§ 10 ratings: {users_with_le_10_ratings:,} ({pct_le_10:.2f}%)\")\n",
    "\n",
    "if pct_le_5 > 50:\n",
    "    print(\"  ‚ö† High cold start risk - many users have minimal interaction\")\n",
    "else:\n",
    "    print(\"  ‚úì Moderate cold start problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4807278d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User activity categories\n",
    "very_active = np.sum(user_rating_counts > 50)\n",
    "active = np.sum((user_rating_counts > 20) & (user_rating_counts <= 50))\n",
    "moderate = np.sum((user_rating_counts > 10) & (user_rating_counts <= 20))\n",
    "occasional = np.sum((user_rating_counts > 5) & (user_rating_counts <= 10))\n",
    "rare = np.sum((user_rating_counts > 1) & (user_rating_counts <= 5))\n",
    "one_time = np.sum(user_rating_counts == 1)\n",
    "\n",
    "print(\"User Activity Categories:\")\n",
    "print(f\"  Very Active (>50): {very_active:,} ({(very_active/n_users)*100:.2f}%)\")\n",
    "print(f\"  Active (21-50): {active:,} ({(active/n_users)*100:.2f}%)\")\n",
    "print(f\"  Moderate (11-20): {moderate:,} ({(moderate/n_users)*100:.2f}%)\")\n",
    "print(f\"  Occasional (6-10): {occasional:,} ({(occasional/n_users)*100:.2f}%)\")\n",
    "print(f\"  Rare (2-5): {rare:,} ({(rare/n_users)*100:.2f}%)\")\n",
    "print(f\"  One-time (1): {one_time:,} ({(one_time/n_users)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0abc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize user activity\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('User Activity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Histogram - ratings per user\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(user_rating_counts, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(mean_ratings_per_user, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {mean_ratings_per_user:.1f}')\n",
    "ax1.axvline(median_ratings_per_user, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {median_ratings_per_user:.1f}')\n",
    "ax1.set_xlabel('Number of Ratings', fontsize=12)\n",
    "ax1.set_ylabel('Number of Users', fontsize=12)\n",
    "ax1.set_title('Distribution of Ratings per User', fontsize=13, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log-scale histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(user_rating_counts, bins=np.logspace(0, np.log10(max_ratings_per_user+1), 50),\n",
    "        color='coral', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlabel('Number of Ratings (log)', fontsize=12)\n",
    "ax2.set_ylabel('Number of Users (log)', fontsize=12)\n",
    "ax2.set_title('User Activity (Log-Log Scale)', fontsize=13, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Top 10 most active users\n",
    "ax3 = axes[1, 0]\n",
    "x_pos = np.arange(len(top_10_counts))\n",
    "bars = ax3.barh(x_pos, top_10_counts, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax3.set_yticks(x_pos)\n",
    "ax3.set_yticklabels([f'User {int(uid)}' for uid in top_10_users], fontsize=9)\n",
    "ax3.set_xlabel('Number of Ratings', fontsize=12)\n",
    "ax3.set_title('Top 10 Most Active Users', fontsize=13, fontweight='bold')\n",
    "ax3.invert_yaxis()\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, top_10_counts)):\n",
    "    ax3.text(count + max(top_10_counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{count:,}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Activity categories pie chart\n",
    "ax4 = axes[1, 1]\n",
    "category_names = ['Very Active\\n(>50)', 'Active\\n(21-50)', 'Moderate\\n(11-20)', \n",
    "                  'Occasional\\n(6-10)', 'Rare\\n(2-5)', 'One-time\\n(1)']\n",
    "category_counts = np.array([very_active, active, moderate, occasional, rare, one_time])\n",
    "colors4 = ['#2ecc71', '#3498db', '#f39c12', '#e67e22', '#e74c3c', '#95a5a6']\n",
    "\n",
    "ax4.pie(category_counts, labels=category_names,\n",
    "        autopct=lambda pct: f'{pct:.1f}%\\n({int(pct/100*n_users):,})',\n",
    "        colors=colors4, startangle=90,\n",
    "        textprops={'fontsize': 9, 'weight': 'bold'})\n",
    "ax4.set_title('User Activity Categories', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f40050c",
   "metadata": {},
   "source": [
    "### 7.2. User Rating Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb32313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate user rating statistics (vectorized)\n",
    "unique_users_arr, inverse_indices = np.unique(user_ids, return_inverse=True)\n",
    "n_users_total = len(unique_users_arr)\n",
    "\n",
    "# Count ratings per user\n",
    "user_n_ratings = np.bincount(inverse_indices)\n",
    "\n",
    "# Calculate mean ratings per user\n",
    "rating_sums = np.bincount(inverse_indices, weights=ratings)\n",
    "user_mean_ratings = rating_sums / user_n_ratings\n",
    "\n",
    "# Calculate std per user\n",
    "rating_squared_sums = np.bincount(inverse_indices, weights=ratings**2)\n",
    "mean_squared = rating_squared_sums / user_n_ratings\n",
    "user_variance = mean_squared - (user_mean_ratings ** 2)\n",
    "user_std_ratings = np.sqrt(np.maximum(user_variance, 0))\n",
    "\n",
    "# Overall statistics\n",
    "overall_mean = np.mean(user_mean_ratings)\n",
    "overall_median = np.median(user_mean_ratings)\n",
    "\n",
    "print(\"User Rating Behavior Statistics:\")\n",
    "print(f\"  Mean of user avg ratings: {overall_mean:.4f}\")\n",
    "print(f\"  Median of user avg ratings: {overall_median:.4f}\")\n",
    "print(f\"  Min user average: {np.min(user_mean_ratings):.4f}\")\n",
    "print(f\"  Max user average: {np.max(user_mean_ratings):.4f}\")\n",
    "\n",
    "# User rating tendencies\n",
    "harsh_raters = np.sum(user_mean_ratings < 3.0)\n",
    "generous_raters = np.sum(user_mean_ratings > 4.0)\n",
    "balanced_raters = np.sum((user_mean_ratings >= 3.0) & (user_mean_ratings <= 4.0))\n",
    "\n",
    "harsh_pct = (harsh_raters / n_users_total) * 100\n",
    "generous_pct = (generous_raters / n_users_total) * 100\n",
    "balanced_pct = (balanced_raters / n_users_total) * 100\n",
    "\n",
    "print(f\"\\nUser Rating Tendencies:\")\n",
    "print(f\"  Harsh (<3.0): {harsh_raters:,} ({harsh_pct:.2f}%)\")\n",
    "print(f\"  Balanced (3.0-4.0): {balanced_raters:,} ({balanced_pct:.2f}%)\")\n",
    "print(f\"  Generous (>4.0): {generous_raters:,} ({generous_pct:.2f}%)\")\n",
    "\n",
    "# Rating consistency\n",
    "users_with_multiple = user_n_ratings > 1\n",
    "filtered_std = user_std_ratings[users_with_multiple]\n",
    "n_users_multiple = np.sum(users_with_multiple)\n",
    "\n",
    "consistent_users = np.sum(filtered_std < 0.5)\n",
    "moderate_users = np.sum((filtered_std >= 0.5) & (filtered_std < 1.0))\n",
    "diverse_users = np.sum(filtered_std >= 1.0)\n",
    "\n",
    "print(f\"\\nUser Rating Consistency (users with >1 rating):\")\n",
    "print(f\"  Consistent (std<0.5): {consistent_users:,} ({(consistent_users/n_users_multiple)*100:.2f}%)\")\n",
    "print(f\"  Moderate (0.5-1.0): {moderate_users:,} ({(moderate_users/n_users_multiple)*100:.2f}%)\")\n",
    "print(f\"  Diverse (std‚â•1.0): {diverse_users:,} ({(diverse_users/n_users_multiple)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7ab3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize user rating behavior\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('User Rating Behavior Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribution of user mean ratings\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(user_mean_ratings, bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(overall_mean, color='red', linestyle='--', linewidth=2, label=f'Mean: {overall_mean:.2f}')\n",
    "ax1.axvline(3.0, color='orange', linestyle=':', linewidth=2, alpha=0.7, label='Harsh threshold')\n",
    "ax1.axvline(4.0, color='purple', linestyle=':', linewidth=2, alpha=0.7, label='Generous threshold')\n",
    "ax1.set_xlabel('Average Rating per User', fontsize=11)\n",
    "ax1.set_ylabel('Number of Users', fontsize=11)\n",
    "ax1.set_title('Distribution of User Average Ratings', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of user std\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(user_std_ratings, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Std Deviation per User', fontsize=11)\n",
    "ax2.set_ylabel('Number of Users', fontsize=11)\n",
    "ax2.set_title('Distribution of User Rating Variability', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Mean vs Std (sample)\n",
    "ax3 = axes[1, 0]\n",
    "sample_size = min(5000, n_users_total)\n",
    "sample_indices = np.random.choice(n_users_total, sample_size, replace=False)\n",
    "scatter = ax3.scatter(user_mean_ratings[sample_indices], user_std_ratings[sample_indices], \n",
    "                     alpha=0.3, s=10, c=user_n_ratings[sample_indices], cmap='viridis')\n",
    "ax3.set_xlabel('Mean Rating', fontsize=11)\n",
    "ax3.set_ylabel('Std Deviation', fontsize=11)\n",
    "ax3.set_title(f'Mean vs Std (sample: {sample_size:,})', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=ax3, label='# Ratings')\n",
    "\n",
    "# User types pie chart\n",
    "ax4 = axes[1, 1]\n",
    "behavior_names = ['Harsh\\n(<3.0)', 'Balanced\\n(3.0-4.0)', 'Generous\\n(>4.0)']\n",
    "behavior_counts = np.array([harsh_raters, balanced_raters, generous_raters])\n",
    "behavior_colors = ['#e74c3c', '#f39c12', '#2ecc71']\n",
    "\n",
    "ax4.pie(behavior_counts, labels=behavior_names,\n",
    "        autopct=lambda pct: f'{pct:.1f}%\\n({int(pct/100*n_users_total):,})',\n",
    "        colors=behavior_colors, startangle=90,\n",
    "        textprops={'fontsize': 10, 'weight': 'bold'})\n",
    "ax4.set_title('User Rating Tendency', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9737d27",
   "metadata": {},
   "source": [
    "## 8. Product Analysis\n",
    "\n",
    "Ph√¢n t√≠ch ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa474f",
   "metadata": {},
   "source": [
    "### 8.1. Product Popularity Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db636d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ratings per product\n",
    "product_ids = data[:, 1]\n",
    "unique_products, product_rating_counts = np.unique(product_ids, return_counts=True)\n",
    "\n",
    "n_products = len(unique_products)\n",
    "total_ratings = len(product_ids)\n",
    "\n",
    "# Statistics\n",
    "mean_ratings_per_product = np.mean(product_rating_counts)\n",
    "median_ratings_per_product = np.median(product_rating_counts)\n",
    "std_ratings_per_product = np.std(product_rating_counts)\n",
    "min_ratings_per_product = np.min(product_rating_counts)\n",
    "max_ratings_per_product = np.max(product_rating_counts)\n",
    "\n",
    "print(\"Product Popularity Statistics:\")\n",
    "print(f\"  Total unique products: {n_products:,}\")\n",
    "print(f\"  Mean ratings per product: {mean_ratings_per_product:.2f}\")\n",
    "print(f\"  Median ratings per product: {median_ratings_per_product:.1f}\")\n",
    "print(f\"  Std deviation: {std_ratings_per_product:.2f}\")\n",
    "print(f\"  Range: {min_ratings_per_product} to {max_ratings_per_product}\")\n",
    "\n",
    "# Top 20 most reviewed products\n",
    "top_20_indices = np.argsort(product_rating_counts)[-20:][::-1]\n",
    "top_20_products = unique_products[top_20_indices]\n",
    "top_20_counts = product_rating_counts[top_20_indices]\n",
    "\n",
    "print(f\"\\nTop 20 Most Reviewed Products:\")\n",
    "print(f\"{'Rank':<6} {'Product ID':<15} {'# Ratings':<15} {'% of Total'}\")\n",
    "print(\"-\" * 50)\n",
    "for rank, (product_id, count) in enumerate(zip(top_20_products, top_20_counts), 1):\n",
    "    pct = (count / total_ratings) * 100\n",
    "    print(f\"{rank:<6} {int(product_id):<15} {count:<15,} {pct:.4f}%\")\n",
    "\n",
    "# Cold start problem\n",
    "products_with_lt_5_ratings = np.sum(product_rating_counts < 5)\n",
    "products_with_lt_10_ratings = np.sum(product_rating_counts < 10)\n",
    "\n",
    "pct_lt_5 = (products_with_lt_5_ratings / n_products) * 100\n",
    "pct_lt_10 = (products_with_lt_10_ratings / n_products) * 100\n",
    "\n",
    "print(f\"\\nProduct Cold Start Analysis:\")\n",
    "print(f\"  Products with < 5 ratings: {products_with_lt_5_ratings:,} ({pct_lt_5:.2f}%)\")\n",
    "print(f\"  Products with < 10 ratings: {products_with_lt_10_ratings:,} ({pct_lt_10:.2f}%)\")\n",
    "\n",
    "if pct_lt_5 > 50:\n",
    "    print(\"  ‚ö† High cold start risk for products\")\n",
    "elif pct_lt_5 > 30:\n",
    "    print(\"  ‚ö† Moderate cold start problem\")\n",
    "else:\n",
    "    print(\"  ‚úì Low cold start problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d391fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product popularity categories\n",
    "blockbuster = np.sum(product_rating_counts > 100)\n",
    "popular = np.sum((product_rating_counts > 50) & (product_rating_counts <= 100))\n",
    "moderate = np.sum((product_rating_counts >= 20) & (product_rating_counts <= 50))\n",
    "niche = np.sum((product_rating_counts >= 10) & (product_rating_counts < 20))\n",
    "unpopular = np.sum((product_rating_counts >= 5) & (product_rating_counts < 10))\n",
    "rare = np.sum(product_rating_counts < 5)\n",
    "\n",
    "print(\"Product Popularity Categories:\")\n",
    "print(f\"  Blockbuster (>100): {blockbuster:,} ({(blockbuster/n_products)*100:.2f}%)\")\n",
    "print(f\"  Popular (51-100): {popular:,} ({(popular/n_products)*100:.2f}%)\")\n",
    "print(f\"  Moderate (20-50): {moderate:,} ({(moderate/n_products)*100:.2f}%)\")\n",
    "print(f\"  Niche (10-19): {niche:,} ({(niche/n_products)*100:.2f}%)\")\n",
    "print(f\"  Unpopular (5-9): {unpopular:,} ({(unpopular/n_products)*100:.2f}%)\")\n",
    "print(f\"  Rare (<5): {rare:,} ({(rare/n_products)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843aa18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize product popularity\n",
    "fig, axes = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "# Top 20 bar chart\n",
    "\n",
    "x_pos = np.arange(len(top_20_counts))\n",
    "bars = axes.barh(x_pos, top_20_counts, color=\"#389379\", alpha=0.7, edgecolor='black')\n",
    "axes.set_yticks(x_pos)\n",
    "axes.set_yticklabels([f'Prod {int(pid)}' for pid in top_20_products], fontsize=9)\n",
    "axes.set_xlabel('Number of Ratings', fontsize=12)\n",
    "axes.set_title('Top 20 Most Reviewed Products', fontsize=13, fontweight='bold')\n",
    "axes.invert_yaxis()\n",
    "axes.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "for i, (bar, count) in enumerate(zip(bars, top_20_counts)):\n",
    "    axes.text(count + max(top_20_counts)*0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{count:,}', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc91b3",
   "metadata": {},
   "source": [
    "### 8.2. Product Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf17e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate product rating statistics (vectorized)\n",
    "unique_products_sorted, inverse_indices_products = np.unique(product_ids, return_inverse=True)\n",
    "n_products_total = len(unique_products_sorted)\n",
    "\n",
    "# Number of ratings per product\n",
    "product_n_ratings = np.bincount(inverse_indices_products)\n",
    "\n",
    "# Mean ratings per product\n",
    "rating_sums_products = np.bincount(inverse_indices_products, weights=ratings)\n",
    "product_mean_ratings = rating_sums_products / product_n_ratings\n",
    "\n",
    "# Std ratings per product\n",
    "rating_squared_sums_products = np.bincount(inverse_indices_products, weights=ratings**2)\n",
    "mean_squared_products = rating_squared_sums_products / product_n_ratings\n",
    "product_variance = mean_squared_products - (product_mean_ratings ** 2)\n",
    "product_std_ratings = np.sqrt(np.maximum(product_variance, 0))\n",
    "\n",
    "# Overall statistics\n",
    "overall_mean_quality = np.mean(product_mean_ratings)\n",
    "overall_median_quality = np.median(product_mean_ratings)\n",
    "\n",
    "print(\"Product Quality Statistics:\")\n",
    "print(f\"  Mean of product avg ratings: {overall_mean_quality:.4f}\")\n",
    "print(f\"  Median of product avg ratings: {overall_median_quality:.4f}\")\n",
    "print(f\"  Min product average: {np.min(product_mean_ratings):.4f}\")\n",
    "print(f\"  Max product average: {np.max(product_mean_ratings):.4f}\")\n",
    "\n",
    "# Quality categories\n",
    "excellent = np.sum(product_mean_ratings >= 4.5)\n",
    "good = np.sum((product_mean_ratings >= 4.0) & (product_mean_ratings < 4.5))\n",
    "average = np.sum((product_mean_ratings >= 3.0) & (product_mean_ratings < 4.0))\n",
    "poor = np.sum((product_mean_ratings >= 2.0) & (product_mean_ratings < 3.0))\n",
    "very_poor = np.sum(product_mean_ratings < 2.0)\n",
    "\n",
    "print(f\"\\nProduct Quality Categories:\")\n",
    "print(f\"  Excellent (‚â•4.5): {excellent:,} ({(excellent/n_products_total)*100:.2f}%)\")\n",
    "print(f\"  Good (4.0-4.5): {good:,} ({(good/n_products_total)*100:.2f}%)\")\n",
    "print(f\"  Average (3.0-4.0): {average:,} ({(average/n_products_total)*100:.2f}%)\")\n",
    "print(f\"  Poor (2.0-3.0): {poor:,} ({(poor/n_products_total)*100:.2f}%)\")\n",
    "print(f\"  Very Poor (<2.0): {very_poor:,} ({(very_poor/n_products_total)*100:.2f}%)\")\n",
    "\n",
    "# Controversy analysis\n",
    "products_with_multiple = product_n_ratings > 1\n",
    "filtered_product_std = product_std_ratings[products_with_multiple]\n",
    "n_products_multiple = np.sum(products_with_multiple)\n",
    "\n",
    "controversial = np.sum(filtered_product_std >= 1.5)\n",
    "mixed = np.sum((filtered_product_std >= 1.0) & (filtered_product_std < 1.5))\n",
    "consistent = np.sum(filtered_product_std < 1.0)\n",
    "\n",
    "print(f\"\\nProduct Rating Controversy (products with >1 rating):\")\n",
    "print(f\"  Consistent (std<1.0): {consistent:,} ({(consistent/n_products_multiple)*100:.2f}%)\")\n",
    "print(f\"  Mixed (1.0-1.5): {mixed:,} ({(mixed/n_products_multiple)*100:.2f}%)\")\n",
    "print(f\"  Controversial (std‚â•1.5): {controversial:,} ({(controversial/n_products_multiple)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499d80ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize product quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Product Quality Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Distribution of product mean ratings\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(product_mean_ratings, bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(overall_mean_quality, color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {overall_mean_quality:.2f}')\n",
    "ax1.axvline(overall_median_quality, color='green', linestyle='--', linewidth=2,\n",
    "           label=f'Median: {overall_median_quality:.2f}')\n",
    "ax1.set_xlabel('Average Rating per Product', fontsize=11)\n",
    "ax1.set_ylabel('Number of Products', fontsize=11)\n",
    "ax1.set_title('Distribution of Product Average Ratings', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution of product std\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(product_std_ratings, bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Std Deviation per Product', fontsize=11)\n",
    "ax2.set_ylabel('Number of Products', fontsize=11)\n",
    "ax2.set_title('Distribution of Product Rating Variability', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter: Popularity vs Quality\n",
    "ax3 = axes[1, 0]\n",
    "eligible_mask = product_n_ratings >= 10\n",
    "if np.sum(eligible_mask) > 0:\n",
    "    eligible_counts = product_n_ratings[eligible_mask]\n",
    "    eligible_means = product_mean_ratings[eligible_mask]\n",
    "    eligible_stds = product_std_ratings[eligible_mask]\n",
    "    \n",
    "    scatter = ax3.scatter(eligible_counts, eligible_means, \n",
    "                         alpha=0.5, s=30, c=eligible_stds, cmap='RdYlGn_r',\n",
    "                         edgecolors='black', linewidth=0.5)\n",
    "    ax3.set_xscale('log')\n",
    "    ax3.set_xlabel('Number of Ratings (log)', fontsize=11)\n",
    "    ax3.set_ylabel('Average Rating', fontsize=11)\n",
    "    ax3.set_title('Popularity vs Quality (min 10 ratings)', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.axhline(overall_mean_quality, color='red', linestyle='--', alpha=0.5, linewidth=1.5)\n",
    "    plt.colorbar(scatter, ax=ax3, label='Std Dev')\n",
    "\n",
    "# Quality categories pie chart\n",
    "ax4 = axes[1, 1]\n",
    "quality_names = ['Excellent\\n(‚â•4.5)', 'Good\\n(4.0-4.5)', 'Average\\n(3.0-4.0)', \n",
    "                 'Poor\\n(2.0-3.0)', 'Very Poor\\n(<2.0)']\n",
    "quality_counts = np.array([excellent, good, average, poor, very_poor])\n",
    "quality_colors = ['#27ae60', '#2ecc71', '#f39c12', '#e67e22', '#e74c3c']\n",
    "\n",
    "# Filter non-zero\n",
    "non_zero_mask = quality_counts > 0\n",
    "filtered_names = [name for name, count in zip(quality_names, quality_counts) if count > 0]\n",
    "filtered_counts = quality_counts[non_zero_mask]\n",
    "filtered_colors = [color for color, count in zip(quality_colors, quality_counts) if count > 0]\n",
    "\n",
    "ax4.pie(filtered_counts, labels=filtered_names,\n",
    "        autopct=lambda pct: f'{pct:.1f}%\\n({int(pct/100*n_products_total):,})',\n",
    "        colors=filtered_colors, startangle=90,\n",
    "        textprops={'fontsize': 9, 'weight': 'bold'})\n",
    "ax4.set_title('Product Quality Categories', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529baefe",
   "metadata": {},
   "source": [
    "## 9. Temporal Analysis\n",
    "\n",
    "Ph√¢n t√≠ch xu h∆∞·ªõng theo th·ªùi gian (n·∫øu c√≥ timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf267699",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"9. TEMPORAL ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Extract timestamps column\n",
    "timestamps = data[:, 3]\n",
    "\n",
    "# Check if timestamps are available\n",
    "non_zero_timestamps = np.sum(timestamps > 0)\n",
    "timestamp_availability = (non_zero_timestamps / len(timestamps)) * 100\n",
    "\n",
    "print(f\"\\nüìÖ Timestamp Availability:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ‚Ä¢ Total ratings: {len(timestamps):,}\")\n",
    "print(f\"  ‚Ä¢ Ratings with valid timestamps: {non_zero_timestamps:,} ({timestamp_availability:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ Ratings without timestamps: {len(timestamps) - non_zero_timestamps:,}\")\n",
    "\n",
    "if non_zero_timestamps > 0:\n",
    "    # Filter ratings with valid timestamps\n",
    "    valid_timestamp_mask = timestamps > 0\n",
    "    valid_timestamps = timestamps[valid_timestamp_mask]\n",
    "    valid_ratings = ratings[valid_timestamp_mask]\n",
    "    \n",
    "    # Convert timestamps to datetime\n",
    "    from datetime import datetime\n",
    "    \n",
    "    # Find timestamp range\n",
    "    min_timestamp = int(np.min(valid_timestamps))\n",
    "    max_timestamp = int(np.max(valid_timestamps))\n",
    "    \n",
    "    min_date = datetime.fromtimestamp(min_timestamp)\n",
    "    max_date = datetime.fromtimestamp(max_timestamp)\n",
    "    \n",
    "    print(f\"\\nüìä Temporal Coverage:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  ‚Ä¢ First rating: {min_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  ‚Ä¢ Last rating: {max_date.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"  ‚Ä¢ Time span: {(max_timestamp - min_timestamp) / (365.25 * 24 * 3600):.2f} years\")\n",
    "    \n",
    "    # Convert timestamps to years and months for aggregation\n",
    "    dates = np.array([datetime.fromtimestamp(int(ts)) for ts in valid_timestamps])\n",
    "    years = np.array([d.year for d in dates])\n",
    "    months = np.array([d.month for d in dates])\n",
    "    year_months = np.array([d.year * 100 + d.month for d in dates])  # YYYYMM format\n",
    "    \n",
    "    # 1. Ratings count over time (by month)\n",
    "    unique_year_months = np.unique(year_months)\n",
    "    monthly_counts = np.array([np.sum(year_months == ym) for ym in unique_year_months])\n",
    "    \n",
    "    # 2. Average rating over time (by month)\n",
    "    monthly_avg_ratings = np.array([np.mean(valid_ratings[year_months == ym]) for ym in unique_year_months])\n",
    "    \n",
    "    print(f\"\\nüìà Activity Trends:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  ‚Ä¢ Unique months with data: {len(unique_year_months)}\")\n",
    "    print(f\"  ‚Ä¢ Peak month: {unique_year_months[np.argmax(monthly_counts)]} with {np.max(monthly_counts):,} ratings\")\n",
    "    print(f\"  ‚Ä¢ Average ratings per month: {np.mean(monthly_counts):.2f}\")\n",
    "    print(f\"  ‚Ä¢ Median ratings per month: {np.median(monthly_counts):.2f}\")\n",
    "    \n",
    "    # 3. Rating inflation analysis\n",
    "    first_half_mask = valid_timestamps < (min_timestamp + (max_timestamp - min_timestamp) / 2)\n",
    "    second_half_mask = ~first_half_mask\n",
    "    \n",
    "    first_half_avg = np.mean(valid_ratings[first_half_mask])\n",
    "    second_half_avg = np.mean(valid_ratings[second_half_mask])\n",
    "    rating_change = second_half_avg - first_half_avg\n",
    "    \n",
    "    print(f\"\\nüîç Rating Inflation Analysis:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  ‚Ä¢ Average rating (first half): {first_half_avg:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Average rating (second half): {second_half_avg:.4f}\")\n",
    "    print(f\"  ‚Ä¢ Change: {rating_change:+.4f}\")\n",
    "    \n",
    "    if abs(rating_change) > 0.1:\n",
    "        if rating_change > 0:\n",
    "            print(\"  ‚ö† Rating inflation detected! Ratings increased over time.\")\n",
    "        else:\n",
    "            print(\"  ‚ö† Rating deflation detected! Ratings decreased over time.\")\n",
    "    else:\n",
    "        print(\"  ‚úì Ratings are relatively stable over time.\")\n",
    "    \n",
    "    # 4. Yearly analysis\n",
    "    unique_years = np.unique(years)\n",
    "    yearly_counts = np.array([np.sum(years == y) for y in unique_years])\n",
    "    yearly_avg_ratings = np.array([np.mean(valid_ratings[years == y]) for y in unique_years])\n",
    "    \n",
    "    print(f\"\\nüìÖ Yearly Breakdown:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Year':<8} {'# Ratings':<15} {'Avg Rating':<15} {'% of Total'}\")\n",
    "    print(\"-\" * 70)\n",
    "    for year, count, avg_rating in zip(unique_years, yearly_counts, yearly_avg_ratings):\n",
    "        pct = (count / len(valid_timestamps)) * 100\n",
    "        print(f\"{int(year):<8} {count:<15,} {avg_rating:<15.4f} {pct:.2f}%\")\n",
    "    \n",
    "    # 5. Seasonal patterns (monthly aggregation)\n",
    "    monthly_distribution = np.array([np.sum(months == m) for m in range(1, 13)])\n",
    "    month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "                   'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    \n",
    "    peak_month = np.argmax(monthly_distribution) + 1\n",
    "    print(f\"\\nüå°Ô∏è Seasonal Patterns:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"  ‚Ä¢ Peak month: {month_names[peak_month-1]} with {monthly_distribution[peak_month-1]:,} ratings\")\n",
    "    print(f\"  ‚Ä¢ Lowest month: {month_names[np.argmin(monthly_distribution)]} with {np.min(monthly_distribution):,} ratings\")\n",
    "    \n",
    "    # Visualizations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "    fig.suptitle('Temporal Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Subplot 1: Ratings count over time (monthly)\n",
    "    ax1 = axes[0, 0]\n",
    "    # Convert YYYYMM to readable format for plotting\n",
    "    month_labels = [f\"{ym//100}-{ym%100:02d}\" for ym in unique_year_months]\n",
    "    # Plot every Nth label to avoid crowding\n",
    "    label_step = max(1, len(month_labels) // 10)\n",
    "    x_indices = np.arange(len(monthly_counts))\n",
    "    \n",
    "    ax1.plot(x_indices, monthly_counts, color='steelblue', linewidth=2)\n",
    "    ax1.fill_between(x_indices, monthly_counts, alpha=0.3, color='steelblue')\n",
    "    ax1.set_xlabel('Time', fontsize=11)\n",
    "    ax1.set_ylabel('Number of Ratings', fontsize=11)\n",
    "    ax1.set_title('Rating Volume Over Time (Monthly)', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xticks(x_indices[::label_step])\n",
    "    ax1.set_xticklabels(month_labels[::label_step], rotation=45, ha='right', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 2: Average rating over time (monthly)\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(x_indices, monthly_avg_ratings, color='#e74c3c', linewidth=2, marker='o', markersize=3)\n",
    "    ax2.axhline(np.mean(valid_ratings), color='green', linestyle='--', linewidth=2, \n",
    "               label=f'Overall avg: {np.mean(valid_ratings):.2f}', alpha=0.7)\n",
    "    ax2.set_xlabel('Time', fontsize=11)\n",
    "    ax2.set_ylabel('Average Rating', fontsize=11)\n",
    "    ax2.set_title('Average Rating Trend Over Time', fontsize=12, fontweight='bold')\n",
    "    ax2.set_xticks(x_indices[::label_step])\n",
    "    ax2.set_xticklabels(month_labels[::label_step], rotation=45, ha='right', fontsize=8)\n",
    "    ax2.set_ylim([0, 5.5])\n",
    "    ax2.legend(fontsize=9)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 3: Yearly distribution\n",
    "    ax3 = axes[0, 2]\n",
    "    bars = ax3.bar(unique_years.astype(str), yearly_counts, color='#9b59b6', alpha=0.7, edgecolor='black')\n",
    "    ax3.set_xlabel('Year', fontsize=11)\n",
    "    ax3.set_ylabel('Number of Ratings', fontsize=11)\n",
    "    ax3.set_title('Ratings Distribution by Year', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, count in zip(bars, yearly_counts):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{count:,}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Subplot 4: Seasonal patterns (monthly distribution)\n",
    "    ax4 = axes[1, 0]\n",
    "    bars4 = ax4.bar(month_names, monthly_distribution, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    ax4.set_xlabel('Month', fontsize=11)\n",
    "    ax4.set_ylabel('Total Ratings', fontsize=11)\n",
    "    ax4.set_title('Seasonal Pattern (All Years Combined)', fontsize=12, fontweight='bold')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Highlight peak month\n",
    "    bars4[peak_month-1].set_color('#e74c3c')\n",
    "    \n",
    "    # Subplot 5: Cumulative ratings over time\n",
    "    ax5 = axes[1, 1]\n",
    "    cumulative_counts = np.cumsum(monthly_counts)\n",
    "    ax5.plot(x_indices, cumulative_counts, color='#27ae60', linewidth=2)\n",
    "    ax5.fill_between(x_indices, cumulative_counts, alpha=0.3, color='#27ae60')\n",
    "    ax5.set_xlabel('Time', fontsize=11)\n",
    "    ax5.set_ylabel('Cumulative Ratings', fontsize=11)\n",
    "    ax5.set_title('Cumulative Rating Growth', fontsize=12, fontweight='bold')\n",
    "    ax5.set_xticks(x_indices[::label_step])\n",
    "    ax5.set_xticklabels(month_labels[::label_step], rotation=45, ha='right', fontsize=8)\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Subplot 6: Rating distribution comparison (first vs second half)\n",
    "    ax6 = axes[1, 2]\n",
    "    first_half_ratings = valid_ratings[first_half_mask]\n",
    "    second_half_ratings = valid_ratings[second_half_mask]\n",
    "    \n",
    "    rating_values = [1, 2, 3, 4, 5]\n",
    "    first_half_dist = [np.sum(first_half_ratings == r) / len(first_half_ratings) * 100 for r in rating_values]\n",
    "    second_half_dist = [np.sum(second_half_ratings == r) / len(second_half_ratings) * 100 for r in rating_values]\n",
    "    \n",
    "    x = np.arange(len(rating_values))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax6.bar(x - width/2, first_half_dist, width, label='First Half', \n",
    "                    color='#3498db', alpha=0.7, edgecolor='black')\n",
    "    bars2 = ax6.bar(x + width/2, second_half_dist, width, label='Second Half', \n",
    "                    color='#e74c3c', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax6.set_xlabel('Rating', fontsize=11)\n",
    "    ax6.set_ylabel('Percentage (%)', fontsize=11)\n",
    "    ax6.set_title('Rating Distribution: First vs Second Half', fontsize=12, fontweight='bold')\n",
    "    ax6.set_xticks(x)\n",
    "    ax6.set_xticklabels(rating_values)\n",
    "    ax6.legend(fontsize=10)\n",
    "    ax6.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n‚úì Temporal analysis completed!\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\n‚ö† No valid timestamps available for temporal analysis.\")\n",
    "    print(\"Skipping temporal visualizations.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a62e529",
   "metadata": {},
   "source": [
    "## 10. Data Sparsity Analysis\n",
    "\n",
    "Ph√¢n t√≠ch ƒë·ªô th∆∞a c·ªßa d·ªØ li·ªáu - quan tr·ªçng cho recommendation systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b535ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"10. DATA SPARSITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Get dimensions of user-item matrix\n",
    "n_users = len(np.unique(user_ids))\n",
    "n_products = len(np.unique(product_ids))\n",
    "n_ratings = len(ratings)\n",
    "\n",
    "# Calculate sparsity\n",
    "total_possible_ratings = n_users * n_products\n",
    "sparsity = 1 - (n_ratings / total_possible_ratings)\n",
    "density = (n_ratings / total_possible_ratings) * 100\n",
    "\n",
    "print(f\"\\nüìä User-Item Matrix Dimensions:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ‚Ä¢ Number of users: {n_users:,}\")\n",
    "print(f\"  ‚Ä¢ Number of products: {n_products:,}\")\n",
    "print(f\"  ‚Ä¢ Matrix size: {n_users:,} √ó {n_products:,}\")\n",
    "print(f\"  ‚Ä¢ Total possible ratings: {total_possible_ratings:,}\")\n",
    "\n",
    "print(f\"\\nüï≥Ô∏è Sparsity Metrics:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ‚Ä¢ Actual ratings: {n_ratings:,}\")\n",
    "print(f\"  ‚Ä¢ Sparsity: {sparsity:.8f} ({sparsity*100:.6f}%)\")\n",
    "print(f\"  ‚Ä¢ Density: {density:.8f}%\")\n",
    "print(f\"  ‚Ä¢ Fill ratio: 1 in {int(1/density*100):,} entries\")\n",
    "\n",
    "if sparsity > 0.99:\n",
    "    print(\"  ‚ö† Extremely sparse matrix! Collaborative filtering will be challenging.\")\n",
    "elif sparsity > 0.95:\n",
    "    print(\"  ‚ö† Very sparse matrix. Matrix factorization recommended.\")\n",
    "else:\n",
    "    print(\"  ‚úì Moderate sparsity. Standard CF algorithms should work well.\")\n",
    "\n",
    "# Calculate coverage metrics\n",
    "users_with_ratings = n_users  # All users have at least 1 rating (by definition)\n",
    "products_with_ratings = n_products  # All products have at least 1 rating\n",
    "\n",
    "user_coverage = (users_with_ratings / n_users) * 100\n",
    "product_coverage = (products_with_ratings / n_products) * 100\n",
    "\n",
    "print(f\"\\nüìà Coverage Statistics:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ‚Ä¢ User coverage: {user_coverage:.2f}% ({users_with_ratings:,}/{n_users:,})\")\n",
    "print(f\"  ‚Ä¢ Product coverage: {product_coverage:.2f}% ({products_with_ratings:,}/{n_products:,})\")\n",
    "\n",
    "# Analyze distribution of non-zero entries per user and product\n",
    "# Use the already calculated user_rating_counts and product_rating_counts\n",
    "print(f\"\\nüìä Non-zero Entries Distribution:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"  ‚Ä¢ Avg ratings per user: {np.mean(user_rating_counts):.2f}\")\n",
    "print(f\"  ‚Ä¢ Median ratings per user: {np.median(user_rating_counts):.1f}\")\n",
    "print(f\"  ‚Ä¢ Max ratings per user: {np.max(user_rating_counts):,}\")\n",
    "print(f\"  ‚Ä¢ Avg ratings per product: {np.mean(product_rating_counts):.2f}\")\n",
    "print(f\"  ‚Ä¢ Median ratings per product: {np.median(product_rating_counts):.1f}\")\n",
    "print(f\"  ‚Ä¢ Max ratings per product: {np.max(product_rating_counts):,}\")\n",
    "\n",
    "# Calculate percentiles for better understanding\n",
    "user_percentiles = np.percentile(user_rating_counts, [25, 50, 75, 90, 95, 99])\n",
    "product_percentiles = np.percentile(product_rating_counts, [25, 50, 75, 90, 95, 99])\n",
    "\n",
    "print(f\"\\nüìä Rating Distribution Percentiles:\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Percentile':<12} {'Users':<15} {'Products':<15}\")\n",
    "print(\"-\" * 70)\n",
    "for pct, u_val, p_val in zip([25, 50, 75, 90, 95, 99], user_percentiles, product_percentiles):\n",
    "    print(f\"{pct}th{'':<9} {u_val:<15.1f} {p_val:<15.1f}\")\n",
    "\n",
    "# Implications for recommendation systems\n",
    "print(f\"\\nüéØ Implications for Recommendation Systems:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Long tail analysis\n",
    "users_top_10_pct = np.sum(user_rating_counts >= np.percentile(user_rating_counts, 90))\n",
    "ratings_by_top_users = np.sum(user_rating_counts[user_rating_counts >= np.percentile(user_rating_counts, 90)])\n",
    "pct_ratings_by_top = (ratings_by_top_users / n_ratings) * 100\n",
    "\n",
    "print(f\"  ‚Ä¢ Top 10% most active users: {users_top_10_pct:,} ({(users_top_10_pct/n_users)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ They contribute: {ratings_by_top_users:,} ratings ({pct_ratings_by_top:.2f}%)\")\n",
    "\n",
    "products_top_10_pct = np.sum(product_rating_counts >= np.percentile(product_rating_counts, 90))\n",
    "ratings_for_top_products = np.sum(product_rating_counts[product_rating_counts >= np.percentile(product_rating_counts, 90)])\n",
    "pct_ratings_for_top = (ratings_for_top_products / n_ratings) * 100\n",
    "\n",
    "print(f\"  ‚Ä¢ Top 10% most popular products: {products_top_10_pct:,} ({(products_top_10_pct/n_products)*100:.2f}%)\")\n",
    "print(f\"  ‚Ä¢ They receive: {ratings_for_top_products:,} ratings ({pct_ratings_for_top:.2f}%)\")\n",
    "\n",
    "# Recommendation strategies\n",
    "print(f\"\\nüí° Recommended Strategies:\")\n",
    "print(\"-\" * 70)\n",
    "if sparsity > 0.999:\n",
    "    print(\"  ‚Ä¢ Use Matrix Factorization (SVD, ALS)\")\n",
    "    print(\"  ‚Ä¢ Implement Content-Based Filtering\")\n",
    "    print(\"  ‚Ä¢ Consider Hybrid Approaches\")\n",
    "    print(\"  ‚Ä¢ Use Popularity-Based for cold start\")\n",
    "elif sparsity > 0.99:\n",
    "    print(\"  ‚Ä¢ Matrix Factorization is highly recommended\")\n",
    "    print(\"  ‚Ä¢ Item-based CF may work better than user-based\")\n",
    "    print(\"  ‚Ä¢ Consider dimensionality reduction\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Both user-based and item-based CF viable\")\n",
    "    print(\"  ‚Ä¢ Matrix factorization will improve performance\")\n",
    "    print(\"  ‚Ä¢ Neighborhood-based methods should work\")\n",
    "\n",
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 11))\n",
    "fig.suptitle('Data Sparsity Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Subplot 1: Sparsity visualization\n",
    "ax1 = axes[0, 0]\n",
    "categories = ['Filled', 'Empty']\n",
    "values = [n_ratings, total_possible_ratings - n_ratings]\n",
    "colors = ['#27ae60', '#e74c3c']\n",
    "explode = [0.1, 0]\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(values, labels=categories, autopct='%1.4f%%',\n",
    "                                     colors=colors, explode=explode, startangle=90,\n",
    "                                     textprops={'fontsize': 11, 'weight': 'bold'})\n",
    "ax1.set_title('User-Item Matrix Sparsity', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Subplot 2: Sample heatmap (small subset)\n",
    "ax2 = axes[0, 1]\n",
    "# Create a small sample matrix for visualization\n",
    "sample_size_users = min(50, n_users)\n",
    "sample_size_products = min(50, n_products)\n",
    "\n",
    "# Randomly sample users and products\n",
    "sampled_user_indices = np.random.choice(n_users, sample_size_users, replace=False)\n",
    "sampled_product_indices = np.random.choice(n_products, sample_size_products, replace=False)\n",
    "\n",
    "# Get unique users and products for indexing\n",
    "unique_users_array = np.unique(user_ids)\n",
    "unique_products_array = np.unique(product_ids)\n",
    "\n",
    "sampled_users = unique_users_array[sampled_user_indices]\n",
    "sampled_products = unique_products_array[sampled_product_indices]\n",
    "\n",
    "# Create sample matrix\n",
    "sample_matrix = np.zeros((sample_size_users, sample_size_products))\n",
    "\n",
    "for i, user in enumerate(sampled_users):\n",
    "    user_mask = user_ids == user\n",
    "    user_products = product_ids[user_mask]\n",
    "    user_ratings = ratings[user_mask]\n",
    "    \n",
    "    for j, product in enumerate(sampled_products):\n",
    "        product_mask = user_products == product\n",
    "        if np.any(product_mask):\n",
    "            sample_matrix[i, j] = user_ratings[product_mask][0]\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax2.imshow(sample_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=5)\n",
    "ax2.set_xlabel('Products (sample)', fontsize=11)\n",
    "ax2.set_ylabel('Users (sample)', fontsize=11)\n",
    "ax2.set_title(f'Sample User-Item Matrix ({sample_size_users}√ó{sample_size_products})', \n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax2, label='Rating')\n",
    "\n",
    "# Subplot 3: Ratings per user distribution (log scale)\n",
    "ax3 = axes[0, 2]\n",
    "ax3.hist(user_rating_counts, bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Ratings per User', fontsize=11)\n",
    "ax3.set_ylabel('Number of Users (log scale)', fontsize=11)\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_title('Distribution of Ratings per User', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Ratings per product distribution (log scale)\n",
    "ax4 = axes[1, 0]\n",
    "ax4.hist(product_rating_counts, bins=50, color='#e67e22', alpha=0.7, edgecolor='black')\n",
    "ax4.set_xlabel('Ratings per Product', fontsize=11)\n",
    "ax4.set_ylabel('Number of Products (log scale)', fontsize=11)\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_title('Distribution of Ratings per Product', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Cumulative distribution of user ratings\n",
    "ax5 = axes[1, 1]\n",
    "sorted_user_counts = np.sort(user_rating_counts)[::-1]\n",
    "cumulative_pct = np.cumsum(sorted_user_counts) / np.sum(sorted_user_counts) * 100\n",
    "user_pct = np.arange(1, len(sorted_user_counts) + 1) / len(sorted_user_counts) * 100\n",
    "\n",
    "ax5.plot(user_pct, cumulative_pct, color='#9b59b6', linewidth=2)\n",
    "ax5.axhline(80, color='red', linestyle='--', linewidth=2, alpha=0.7, label='80% of ratings')\n",
    "ax5.axvline(20, color='green', linestyle='--', linewidth=2, alpha=0.7, label='20% of users')\n",
    "ax5.set_xlabel('Cumulative % of Users', fontsize=11)\n",
    "ax5.set_ylabel('Cumulative % of Ratings', fontsize=11)\n",
    "ax5.set_title('Pareto Analysis: User Contribution', fontsize=12, fontweight='bold')\n",
    "ax5.legend(fontsize=10)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Find the % of users contributing 80% of ratings\n",
    "users_for_80_pct = np.searchsorted(cumulative_pct, 80) / len(cumulative_pct) * 100\n",
    "ax5.text(0.5, 0.95, f'{users_for_80_pct:.1f}% of users\\ncontribute 80% of ratings', \n",
    "        transform=ax5.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Subplot 6: Cumulative distribution of product ratings\n",
    "ax6 = axes[1, 2]\n",
    "sorted_product_counts = np.sort(product_rating_counts)[::-1]\n",
    "cumulative_pct_products = np.cumsum(sorted_product_counts) / np.sum(sorted_product_counts) * 100\n",
    "product_pct = np.arange(1, len(sorted_product_counts) + 1) / len(sorted_product_counts) * 100\n",
    "\n",
    "ax6.plot(product_pct, cumulative_pct_products, color='#e74c3c', linewidth=2)\n",
    "ax6.axhline(80, color='red', linestyle='--', linewidth=2, alpha=0.7, label='80% of ratings')\n",
    "ax6.axvline(20, color='green', linestyle='--', linewidth=2, alpha=0.7, label='20% of products')\n",
    "ax6.set_xlabel('Cumulative % of Products', fontsize=11)\n",
    "ax6.set_ylabel('Cumulative % of Ratings', fontsize=11)\n",
    "ax6.set_title('Pareto Analysis: Product Popularity', fontsize=12, fontweight='bold')\n",
    "ax6.legend(fontsize=10)\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Find the % of products receiving 80% of ratings\n",
    "products_for_80_pct = np.searchsorted(cumulative_pct_products, 80) / len(cumulative_pct_products) * 100\n",
    "ax6.text(0.5, 0.95, f'{products_for_80_pct:.1f}% of products\\nreceive 80% of ratings', \n",
    "        transform=ax6.transAxes, fontsize=10, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Data sparsity analysis completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53440392",
   "metadata": {},
   "source": [
    "## 11. Exploratory Questions\n",
    "\n",
    "ƒê·∫∑t v√† tr·∫£ l·ªùi c√°c c√¢u h·ªèi v·ªÅ d·ªØ li·ªáu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccae6d",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 1: C√≥ hi·ªán t∆∞·ª£ng rating polarization kh√¥ng?\n",
    "\n",
    "Ng∆∞·ªùi d√πng c√≥ xu h∆∞·ªõng cho ƒëi·ªÉm c·ª±c cao (5 sao) ho·∫∑c c·ª±c th·∫•p (1 sao), √≠t cho ƒëi·ªÉm trung b√¨nh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fc1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze rating polarization\n",
    "# - Compare frequency of extreme ratings (1, 5) vs middle ratings (2, 3, 4)\n",
    "# - Visualize with bar chart v√† pie chart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d730a91a",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 2: S·∫£n ph·∫©m n·ªïi b·∫≠t nh·∫•t l√† g√¨?\n",
    "\n",
    "X√°c ƒë·ªãnh s·∫£n ph·∫©m c√≥ s·ª± k·∫øt h·ª£p t·ªët gi·ªØa s·ªë l∆∞·ª£ng ratings v√† ch·∫•t l∆∞·ª£ng ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2c285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Find outstanding products\n",
    "# Calculate popularity score: weighted rating\n",
    "# Formula: weighted_rating = (v/(v+m)) * R + (m/(v+m)) * C\n",
    "# Where:\n",
    "# v = number of ratings for the product\n",
    "# m = minimum ratings threshold\n",
    "# R = average rating for the product\n",
    "# C = mean rating across all products\n",
    "\n",
    "# Visualize top products v·ªõi scatter plot (ratings count vs avg rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb84468",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 3: C√≥ t·ªìn t·∫°i \"power users\" kh√¥ng?\n",
    "\n",
    "Ph√¢n t√≠ch s·ª± ph√¢n b·ªë ho·∫°t ƒë·ªông c·ªßa users - c√≥ m·ªôt nh√≥m nh·ªè users ƒë√≥ng g√≥p ph·∫ßn l·ªõn ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaab5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze power users\n",
    "# - Calculate cumulative percentage of ratings\n",
    "# - Create Pareto chart (80-20 rule?)\n",
    "# - What % of users contribute 80% of ratings?\n",
    "\n",
    "# This is important for:\n",
    "# - Understanding data concentration\n",
    "# - Cold start problem severity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4dedf",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 4: C√≥ m·ªëi quan h·ªá gi·ªØa s·ªë l∆∞·ª£ng ratings v√† average rating?\n",
    "\n",
    "S·∫£n ph·∫©m ph·ªï bi·∫øn c√≥ xu h∆∞·ªõng ƒë∆∞·ª£c ƒë√°nh gi√° cao h∆°n kh√¥ng?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf2109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Correlation analysis\n",
    "# - Calculate correlation between popularity and rating\n",
    "# - Scatter plot with trend line\n",
    "# - Statistical significance test\n",
    "\n",
    "# Use NumPy to calculate Pearson correlation:\n",
    "# r = cov(X,Y) / (std(X) * std(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1f27c9",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 5: Cold Start Problem nghi√™m tr·ªçng nh∆∞ th·∫ø n√†o?\n",
    "\n",
    "Ph√¢n t√≠ch s·ªë l∆∞·ª£ng users v√† products c√≥ √≠t t∆∞∆°ng t√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f8446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze cold start problem\n",
    "# - % of users with <= 5 ratings\n",
    "# - % of products with <= 5 ratings\n",
    "# - Distribution of ratings per user/product\n",
    "\n",
    "# Impact on recommendation system:\n",
    "# - Hard to make recommendations for new users/products\n",
    "# - Need different strategies (content-based, popularity-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ae29c",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 6: Ph√¢n kh√∫c ng∆∞·ªùi d√πng (User Segmentation)\n",
    "\n",
    "C√≥ th·ªÉ chia ng∆∞·ªùi d√πng th√†nh c√°c nh√≥m d·ª±a tr√™n h√†nh vi rating kh√¥ng?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c50cf73",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 7: Product Lifecycle - S·∫£n ph·∫©m \"Rising Stars\" v√† \"Falling Stars\"\n",
    "\n",
    "Ph√°t hi·ªán s·∫£n ph·∫©m ƒëang l√™n (trending) v√† s·∫£n ph·∫©m ƒëang xu·ªëng (declining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47868f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ph√¢n t√≠ch product lifecycle n·∫øu c√≥ timestamp\n",
    "# 1. Chia timeline th√†nh periods (v√≠ d·ª•: quarterly)\n",
    "# 2. T√≠nh s·ªë ratings v√† avg rating m·ªói period cho t·ª´ng product\n",
    "# 3. T√≠nh growth rate:\n",
    "#    growth_rate = (recent_ratings - old_ratings) / old_ratings\n",
    "#\n",
    "# Rising Stars:\n",
    "# - Positive growth rate\n",
    "# - Improving average rating\n",
    "# - Increasing review frequency\n",
    "#\n",
    "# Falling Stars:\n",
    "# - Negative growth rate\n",
    "# - Declining ratings\n",
    "# - Decreasing review frequency\n",
    "#\n",
    "# Stable Products:\n",
    "# - Consistent rating volume\n",
    "# - Stable quality\n",
    "\n",
    "# Business Application:\n",
    "# - Inventory management: Stock up on rising stars\n",
    "# - Promotional strategy: Boost falling stars or discontinue\n",
    "# - Recommendation priority: Feature trending products\n",
    "# - New product launch insights\n",
    "\n",
    "# Visualize:\n",
    "# - Line chart: Rating count over time for top rising/falling products\n",
    "# - Scatter: Growth rate vs Current avg rating\n",
    "# - Heatmap: Product performance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbf44e1",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 8: Cross-Product Purchase Patterns\n",
    "\n",
    "C√≥ nh√≥m s·∫£n ph·∫©m n√†o th∆∞·ªùng ƒë∆∞·ª£c mua c√πng nhau kh√¥ng?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a06330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ph√¢n t√≠ch co-occurrence patterns\n",
    "# 1. T·∫°o user-product matrix (binary: rated or not)\n",
    "# 2. Calculate product co-occurrence matrix:\n",
    "#    Co-occurrence(i,j) = number of users who rated both product i and j\n",
    "# 3. Normalize by product popularity\n",
    "# 4. Calculate lift score:\n",
    "#    lift(i,j) = P(i,j) / (P(i) * P(j))\n",
    "#    Where P(i,j) = probability both rated together\n",
    "#          P(i), P(j) = individual probabilities\n",
    "\n",
    "# Find product pairs v·ªõi:\n",
    "# - High co-occurrence count\n",
    "# - High lift score (> 1 means positive association)\n",
    "\n",
    "# Business Application:\n",
    "# - Bundle recommendations: \"Customers who bought X also bought Y\"\n",
    "# - Cross-selling opportunities\n",
    "# - Product placement in store/website\n",
    "# - Combo deals and promotions\n",
    "\n",
    "# Visualize:\n",
    "# - Network graph: Products as nodes, co-occurrence as edges\n",
    "# - Heatmap: Top 30 products co-occurrence matrix\n",
    "# - Bar chart: Top product pairs by lift score\n",
    "\n",
    "# Example insights:\n",
    "# - Shampoo + Conditioner (expected)\n",
    "# - Lipstick + Eye shadow (complementary)\n",
    "# - Face cream + Serum (product line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17f4e19",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 9: Rating Reliability - S·∫£n ph·∫©m n√†o c√≥ ratings ƒë√°ng tin c·∫≠y?\n",
    "\n",
    "ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa ratings d·ª±a tr√™n s·ªë l∆∞·ª£ng v√† consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d41e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate rating reliability score\n",
    "# Reliability factors:\n",
    "# 1. Sample size: More ratings = more reliable\n",
    "# 2. Rating consistency: Low variance = more reliable\n",
    "# 3. Recency: Recent ratings more relevant (if timestamp available)\n",
    "# 4. Reviewer diversity: More unique users = less bias\n",
    "\n",
    "# Calculate Confidence Score:\n",
    "# confidence = (n_ratings / (n_ratings + k)) * consistency_factor\n",
    "# Where:\n",
    "# - k = threshold constant (e.g., 10)\n",
    "# - consistency_factor = 1 / (1 + std_rating)\n",
    "\n",
    "# Identify categories:\n",
    "# - High confidence products: Many ratings, low variance\n",
    "# - Controversial products: Many ratings, high variance\n",
    "# - Uncertain products: Few ratings (need more data)\n",
    "\n",
    "# Calculate Wilson Score Confidence Interval (advanced):\n",
    "# For binary outcomes (positive/negative), gives lower bound of true rating\n",
    "\n",
    "# Business Application:\n",
    "# - Quality control: Flag products with low confidence for review\n",
    "# - Recommendation confidence: Show reliability indicators to users\n",
    "# - Inventory decisions: Prioritize high-confidence high-rated products\n",
    "# - A/B testing: Focus improvement efforts on uncertain products\n",
    "\n",
    "# Visualize:\n",
    "# - Scatter plot: Number of ratings vs Rating variance\n",
    "# - Color by average rating\n",
    "# - Size by confidence score\n",
    "# - Quadrant analysis:\n",
    "#   * Top-right: Popular & Reliable (safe bets)\n",
    "#   * Top-left: Popular & Controversial (investigate)\n",
    "#   * Bottom-right: Niche & Reliable (hidden gems)\n",
    "#   * Bottom-left: Uncertain (need data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb9d7d5",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 10: Market Basket Analysis - Loyalty Patterns\n",
    "\n",
    "Ng∆∞·ªùi d√πng c√≥ trung th√†nh v·ªõi nh√≥m s·∫£n ph·∫©m n√†o? Churn risk ·ªü ƒë√¢u?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c52510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ph√¢n t√≠ch loyalty patterns\n",
    "# 1. User loyalty metrics:\n",
    "#    - Product diversity: Unique products / Total ratings\n",
    "#    - Average rating trend: Early ratings vs Recent ratings\n",
    "#    - Rating frequency: Time between ratings (if timestamp available)\n",
    "#\n",
    "# 2. Identify user types:\n",
    "#    Loyal fans:\n",
    "#    - High ratings consistently (>4.0)\n",
    "#    - Low product diversity (stick to favorites)\n",
    "#    - Regular activity\n",
    "#\n",
    "#    Explorers:\n",
    "#    - High product diversity\n",
    "#    - Variable ratings\n",
    "#    - Frequent activity\n",
    "#\n",
    "#    Churned/At-risk:\n",
    "#    - Declining average ratings over time\n",
    "#    - Increasing time gaps between purchases\n",
    "#    - Recent low ratings\n",
    "#\n",
    "#    One-time buyers:\n",
    "#    - Single rating\n",
    "#    - No return\n",
    "\n",
    "# Calculate churn indicators:\n",
    "# - Time since last rating (recency)\n",
    "# - Negative rating trend\n",
    "# - Comparison with historical behavior\n",
    "\n",
    "# Business Application:\n",
    "# - Retention campaigns: Target at-risk users\n",
    "# - Win-back campaigns: Re-engage churned users\n",
    "# - Loyalty rewards: Incentivize loyal fans\n",
    "# - Product recommendations: \n",
    "#   * Loyal fans ‚Üí Similar products in same category\n",
    "#   * Explorers ‚Üí Diverse recommendations\n",
    "# - Customer lifetime value prediction\n",
    "\n",
    "# Visualize:\n",
    "# - Sankey diagram: User journey through product categories\n",
    "# - Cohort analysis: Rating behavior over user lifecycle\n",
    "# - RFM analysis adapted:\n",
    "#   * Recency: Time since last rating\n",
    "#   * Frequency: Number of ratings\n",
    "#   * Monetary (proxy): Average rating (satisfaction level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba8f438",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 11: Price-Quality Perception (n·∫øu c√≥ d·ªØ li·ªáu gi√°)\n",
    "\n",
    "C√≥ m·ªëi quan h·ªá gi·ªØa rating v√† nh√≥m gi√° s·∫£n ph·∫©m kh√¥ng?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ce0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Price-Quality analysis (if price data available in product metadata)\n",
    "# Note: N·∫øu kh√¥ng c√≥ price data, c√≥ th·ªÉ infer t·ª´ rating patterns\n",
    "#\n",
    "# 1. Categorize products by price tier (if available):\n",
    "#    - Budget: < 25th percentile\n",
    "#    - Mid-range: 25th - 75th percentile\n",
    "#    - Premium: > 75th percentile\n",
    "#\n",
    "# 2. Alternative: Infer \"perceived value\" t·ª´ ratings:\n",
    "#    High-value products: High ratings + Many reviews\n",
    "#    Low-value products: Low ratings despite popularity\n",
    "#\n",
    "# 3. Analyze:\n",
    "#    - Average rating per price tier\n",
    "#    - Rating variance per price tier\n",
    "#    - Customer expectations: Do expensive products need higher ratings?\n",
    "#    - Value for money: High rating + Lower price tier\n",
    "\n",
    "# Calculate \"bang for buck\" score:\n",
    "# value_score = avg_rating / (price_tier_normalized + epsilon)\n",
    "\n",
    "# Identify:\n",
    "# - Overperformers: Budget/Mid-range with premium-level ratings\n",
    "# - Underperformers: Premium with mediocre ratings\n",
    "# - Sweet spots: Best value for money\n",
    "\n",
    "# Business Application:\n",
    "# - Pricing strategy: Adjust prices based on perceived value\n",
    "# - Marketing positioning: Highlight value products\n",
    "# - Premium justification: Ensure premium products deliver quality\n",
    "# - Recommendation diversity: Mix price tiers in recommendations\n",
    "# - Competitive analysis: Compare similar products across tiers\n",
    "\n",
    "# Visualize:\n",
    "# - Box plots: Rating distribution per price tier\n",
    "# - Scatter plot: Price vs Rating (if data available)\n",
    "# - Bar chart: Value score leaders\n",
    "# - Heatmap: Price tier √ó Rating category matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2269d0",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 12: Rating Velocity - Momentum Analysis\n",
    "\n",
    "S·∫£n ph·∫©m n√†o ƒëang ƒë∆∞·ª£c rating nhi·ªÅu ƒë·ªôt ng·ªôt? (Viral products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded6d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze rating velocity and momentum (requires timestamp)\n",
    "# 1. Calculate rating velocity:\n",
    "#    velocity = ratings_in_recent_period / ratings_in_previous_period\n",
    "#    \n",
    "# 2. Calculate acceleration:\n",
    "#    acceleration = change in velocity over time\n",
    "#\n",
    "# 3. Identify patterns:\n",
    "#    Viral products:\n",
    "#    - High velocity (many recent ratings)\n",
    "#    - Positive acceleration (accelerating growth)\n",
    "#    - Sudden spike in activity\n",
    "#\n",
    "#    Steady growers:\n",
    "#    - Consistent positive velocity\n",
    "#    - Stable acceleration\n",
    "#\n",
    "#    Declining products:\n",
    "#    - Negative velocity\n",
    "#    - Negative acceleration\n",
    "#\n",
    "#    Seasonal products:\n",
    "#    - Periodic velocity spikes\n",
    "#    - Predictable patterns\n",
    "\n",
    "# Calculate momentum score:\n",
    "# momentum = (recent_ratings / avg_ratings_per_period) * (recent_avg_rating / overall_avg)\n",
    "\n",
    "# Business Application:\n",
    "# - Trend spotting: Catch viral products early\n",
    "# - Inventory management: Stock up on high-momentum products\n",
    "# - Marketing timing: Promote products at peak momentum\n",
    "# - Recommendation freshness: Feature trending products\n",
    "# - Competitive intelligence: Track momentum vs competitors\n",
    "# - Product launch success: Monitor new product momentum\n",
    "\n",
    "# Visualize:\n",
    "# - Time series: Rating count over time for viral products\n",
    "# - Velocity chart: Rate of change visualization\n",
    "# - Momentum heatmap: Products √ó Time periods\n",
    "# - Acceleration scatter: Current velocity vs acceleration\n",
    "# - Top movers dashboard: Biggest gainers/losers\n",
    "\n",
    "# Real-world examples:\n",
    "# - Holiday season spikes (e.g., gift sets)\n",
    "# - Influencer effects (sudden popularity after review)\n",
    "# - Seasonal trends (sunscreen in summer)\n",
    "# - Competitor product failures (switchers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5583857d",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 13: User Influence Score - Ai l√† Key Opinion Leaders?\n",
    "\n",
    "X√°c ƒë·ªãnh users c√≥ ·∫£nh h∆∞·ªüng cao (ratings c·ªßa h·ªç predict ƒë∆∞·ª£c behaviors c·ªßa others)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2314bd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate user influence scores\n",
    "# Influence factors:\n",
    "# 1. Activity level:\n",
    "#    - Number of ratings\n",
    "#    - Diversity of products rated\n",
    "#\n",
    "# 2. Early adopter behavior:\n",
    "#    - Among first to rate new products (if timestamp)\n",
    "#    - Ratings on niche/less popular items\n",
    "#\n",
    "# 3. Rating impact:\n",
    "#    - Agreement with community: How often user rating aligns with final consensus\n",
    "#    - Predictive power: User rates high ‚Üí product becomes popular\n",
    "#    - Contrarian accuracy: User finds hidden gems others missed\n",
    "#\n",
    "# 4. Expertise indicators:\n",
    "#    - Detailed ratings (if text reviews available)\n",
    "#    - Consistent rating behavior\n",
    "#    - Coverage of product categories\n",
    "\n",
    "# Calculate influence score:\n",
    "# influence = w1 * activity_score + \n",
    "#             w2 * early_adopter_score + \n",
    "#             w3 * prediction_accuracy_score +\n",
    "#             w4 * diversity_score\n",
    "\n",
    "# For prediction accuracy:\n",
    "# - Compare user's early rating vs product's eventual average\n",
    "# - Reward users whose ratings predict future popularity\n",
    "# - Calculate correlation between user rating and product success\n",
    "\n",
    "# Identify user tiers:\n",
    "# - Influencers: High influence, high activity, early adopters\n",
    "# - Experts: High accuracy, niche focus, consistent ratings\n",
    "# - Casual users: Low influence, sporadic activity\n",
    "# - Followers: Late adopters, rate popular items\n",
    "\n",
    "# Business Application:\n",
    "# - Influencer partnerships: Engage high-influence users\n",
    "# - Beta testing: Invite influencers to try new products\n",
    "# - User-generated content: Encourage reviews from experts\n",
    "# - Weighted recommendations: Give more weight to influencer ratings\n",
    "# - Community building: Create expert/influencer badges\n",
    "# - Product seeding: Send samples to key opinion leaders\n",
    "# - Credibility indicators: Show \"Expert rated 4.5‚òÖ\" separately\n",
    "\n",
    "# Visualize:\n",
    "# - Influence distribution: Histogram of influence scores\n",
    "# - Scatter plot: Activity vs Influence (are they correlated?)\n",
    "# - Network graph: User influence relationships\n",
    "# - Leaderboard: Top 50 most influential users\n",
    "# - Influence over time: Track how influence changes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f8129",
   "metadata": {},
   "source": [
    "### C√¢u h·ªèi 14: Recommendation Diversity vs Accuracy Trade-off\n",
    "\n",
    "Balance gi·ªØa recommend s·∫£n ph·∫©m t∆∞∆°ng t·ª± vs kh√°m ph√° s·∫£n ph·∫©m m·ªõi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac98516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze diversity patterns in user behavior\n",
    "# 1. Calculate user exploration behavior:\n",
    "#    Diversity Index = Entropy of product categories rated\n",
    "#    entropy = -Œ£(p_i * log(p_i))\n",
    "#    where p_i = proportion of ratings in category i\n",
    "#\n",
    "# 2. Similarity within user's rated products:\n",
    "#    - Calculate avg pairwise similarity of products user rated\n",
    "#    - Use rating patterns of all users as feature space\n",
    "#    - High similarity = user likes similar products (easy to recommend)\n",
    "#    - Low similarity = diverse taste (hard but interesting)\n",
    "#\n",
    "# 3. Exploration vs Exploitation:\n",
    "#    Exploitation: Rating products similar to past high-rated items\n",
    "#    Exploration: Rating diverse/different products\n",
    "#\n",
    "# 4. Calculate per user:\n",
    "#    - % of ratings on popular products (>100 reviews)\n",
    "#    - % of ratings on niche products (<10 reviews)\n",
    "#    - Sequential pattern: Do they alternate or cluster?\n",
    "\n",
    "# Segment users by recommendation strategy:\n",
    "# - Conservative users (high similarity, low diversity):\n",
    "#   ‚Üí Recommend similar popular items (safe bets)\n",
    "#   ‚Üí Collaborative filtering works well\n",
    "#\n",
    "# - Adventurous users (low similarity, high diversity):\n",
    "#   ‚Üí Recommend diverse items from multiple categories\n",
    "#   ‚Üí Content-based + serendipity important\n",
    "#   ‚Üí \"Because you like variety\" recommendations\n",
    "#\n",
    "# - Balanced users:\n",
    "#   ‚Üí Hybrid approach with some exploration\n",
    "\n",
    "# Business Application:\n",
    "# - Personalized recommendation strategy per user type\n",
    "# - A/B testing: Test diversity levels in recommendations\n",
    "# - User satisfaction: Match recommendation diversity to user preference\n",
    "# - Product discovery: Help users find hidden gems\n",
    "# - Filter bubble avoidance: Prevent over-specialization\n",
    "# - Long-tail promotion: Use diversity-loving users to promote niche items\n",
    "\n",
    "# Calculate system-level metrics:\n",
    "# - Aggregate diversity: How many unique items recommended across all users?\n",
    "# - Coverage: What % of catalog gets recommended?\n",
    "# - Serendipity: How often do users rate high on unexpected recommendations?\n",
    "\n",
    "# Visualize:\n",
    "# - User scatter: Diversity Index vs Average Rating\n",
    "# - Distribution: Histogram of user diversity scores\n",
    "# - Category sunburst: Rating distribution across categories per user type\n",
    "# - Trade-off curve: Recommendation accuracy vs diversity\n",
    "# - Temporal: Does diversity change over user lifecycle?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab02-beauty-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
