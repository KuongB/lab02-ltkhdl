{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. Modeling and Evaluation\n",
                "\n",
                "## Objective\n",
                "Implement and compare three models using **NumPy exclusively**:\n",
                "1.  **Model 1 (Reduced)**: Linear Regression on key features.\n",
                "2.  **Model 2 (Full - Stabilized)**: Ridge Regression on all features (including Advanced Features).\n",
                "3.  **Model 3 (KNN)**: K-Nearest Neighbors Regression.\n",
                "\n",
                "**Evaluation Strategy**:\n",
                "- **Hold-out**: 20% of data reserved for final testing.\n",
                "- **Cross-Validation**: 5-Fold CV on the remaining 80% training data.\n",
                "- **Metrics**: Mean Squared Error (MSE) and $R^2$ Score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading processed data...\n",
                        "Features: ['latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'price_log', 'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', 'neighbourhood_group_Staten Island', 'room_type_Private room', 'room_type_Shared room', 'review_activity', 'review_quality_score']\n",
                        "Data Shape: (48258, 17)\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import csv\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "PROCESSED_DATA_PATH = '../data/processed/airbnb_processed.csv'\n",
                "\n",
                "def load_processed_data(file_path):\n",
                "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
                "        reader = csv.reader(f)\n",
                "        header = next(reader)\n",
                "        data = list(reader)\n",
                "    return header, np.array(data, dtype=float)\n",
                "\n",
                "print(\"Loading processed data...\")\n",
                "header, data = load_processed_data(PROCESSED_DATA_PATH)\n",
                "feature_names = header[:-1]\n",
                "target_name = header[-1]\n",
                "\n",
                "X_all = data[:, :-1]\n",
                "Y_all = data[:, -1]\n",
                "\n",
                "print(\"Features:\", feature_names)\n",
                "print(\"Data Shape:\", X_all.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Splitting\n",
                "Split data into **Train (80%)** and **Test (20%)** sets explicitly."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Set: 38606 samples\n",
                        "Test Set: 9652 samples\n"
                    ]
                }
            ],
            "source": [
                "np.random.seed(42)\n",
                "indices = np.arange(len(X_all))\n",
                "np.random.shuffle(indices)\n",
                "\n",
                "test_size = 0.2\n",
                "split_idx = int(len(X_all) * (1 - test_size))\n",
                "\n",
                "train_indices = indices[:split_idx]\n",
                "test_indices = indices[split_idx:]\n",
                "\n",
                "X_train_full = X_all[train_indices]\n",
                "Y_train = Y_all[train_indices]\n",
                "\n",
                "X_test_full = X_all[test_indices]\n",
                "Y_test = Y_all[test_indices]\n",
                "\n",
                "print(f\"Training Set: {X_train_full.shape[0]} samples\")\n",
                "print(f\"Test Set: {X_test_full.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Models Implementation (Linear & KNN)\n",
                "1. **Linear Regression** (Normal Equation)\n",
                "2. **Ridge Regression** (Stabilized Normal Eq)\n",
                "3. **KNN Regression** (Euclidean Distance)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "def add_intercept(X):\n",
                "    intercept = np.ones((X.shape[0], 1))\n",
                "    return np.hstack((intercept, X))\n",
                "\n",
                "# --- Linear Models ---\n",
                "def train_linear_regression(X, Y):\n",
                "    X_T = X.T\n",
                "    try:\n",
                "        W = np.linalg.inv(X_T @ X) @ X_T @ Y\n",
                "    except np.linalg.LinAlgError:\n",
                "        W = np.linalg.pinv(X_T @ X) @ X_T @ Y\n",
                "    return W\n",
                "\n",
                "def train_ridge_regression(X, Y, lambda_reg=0.001):\n",
                "    X_T = X.T\n",
                "    n_features = X.shape[1]\n",
                "    I = np.eye(n_features)\n",
                "    I[0, 0] = 0 # No reg on intercept\n",
                "    try:\n",
                "        W = np.linalg.inv(X_T @ X + lambda_reg * I) @ X_T @ Y\n",
                "    except np.linalg.LinAlgError:\n",
                "         W = np.linalg.pinv(X_T @ X + lambda_reg * I) @ X_T @ Y\n",
                "    return W\n",
                "\n",
                "def predict_linear(X, W):\n",
                "    return X @ W\n",
                "\n",
                "# --- KNN Model ---\n",
                "class KNNRegressor:\n",
                "    def __init__(self, k=5):\n",
                "        self.k = k\n",
                "        self.X_train = None\n",
                "        self.Y_train = None\n",
                "\n",
                "    def fit(self, X, Y):\n",
                "        self.X_train = X\n",
                "        self.Y_train = Y\n",
                "\n",
                "    def predict(self, X_test):\n",
                "        predictions = []\n",
                "        # Loop through each test point (vectorized distance calc per point)\n",
                "        # For large datasets, this can be slow. We optimize where possible.\n",
                "        for i in range(len(X_test)):\n",
                "            test_point = X_test[i]\n",
                "            \n",
                "            # Euclidean Distance: sqrt(sum((x - y)^2))\n",
                "            # Broadcasting test_point across all X_train\n",
                "            distances = np.sqrt(np.sum((self.X_train - test_point) ** 2, axis=1))\n",
                "            \n",
                "            # Find K nearest indices\n",
                "            # argpartition is faster than sort for finding top k\n",
                "            nearest_indices = np.argpartition(distances, self.k)[:self.k]\n",
                "            \n",
                "            # Get mean of Y values\n",
                "            k_nearest_y = self.Y_train[nearest_indices]\n",
                "            pred = np.mean(k_nearest_y)\n",
                "            predictions.append(pred)\n",
                "            \n",
                "        return np.array(predictions)\n",
                "\n",
                "# --- Metrics ---\n",
                "def mse(y_true, y_pred):\n",
                "    return np.mean((y_true - y_pred) ** 2)\n",
                "\n",
                "def r2_score(y_true, y_pred):\n",
                "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
                "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
                "    return 1 - (ss_res / ss_tot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. K-Fold Cross Validation Helper\n",
                "Supports both Linear Weights and KNN Object approaches."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "def k_fold_cross_validation(X, Y, model_type='linear', k=5, **kwargs):\n",
                "    fold_size = len(X) // k\n",
                "    indices = np.arange(len(X))\n",
                "    \n",
                "    mse_scores = []\n",
                "    r2_scores = []\n",
                "    \n",
                "    print(f\"Starting {k}-Fold CV for {model_type}...\")\n",
                "    \n",
                "    for i in range(k):\n",
                "        val_start = i * fold_size\n",
                "        val_end = (i + 1) * fold_size\n",
                "        \n",
                "        val_idx = indices[val_start:val_end]\n",
                "        train_idx = np.concatenate([indices[:val_start], indices[val_end:]])\n",
                "        \n",
                "        X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
                "        X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
                "        \n",
                "        if model_type == 'linear':\n",
                "            W = train_linear_regression(X_train_fold, Y_train_fold)\n",
                "            Y_pred = predict_linear(X_val_fold, W)\n",
                "        elif model_type == 'ridge':\n",
                "            W = train_ridge_regression(X_train_fold, Y_train_fold, **kwargs)\n",
                "            Y_pred = predict_linear(X_val_fold, W)\n",
                "        elif model_type == 'knn':\n",
                "            knn = KNNRegressor(**kwargs)\n",
                "            knn.fit(X_train_fold, Y_train_fold)\n",
                "            Y_pred = knn.predict(X_val_fold)\n",
                "            \n",
                "        mse_val = mse(Y_val_fold, Y_pred)\n",
                "        r2_val = r2_score(Y_val_fold, Y_pred)\n",
                "        \n",
                "        mse_scores.append(mse_val)\n",
                "        r2_scores.append(r2_val)\n",
                "        # print(f\"Fold {i+1}: MSE={mse_val:.4f}, R2={r2_val:.4f}\")\n",
                "        \n",
                "    return np.mean(mse_scores), np.mean(r2_scores)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Comparisons\n",
                "\n",
                "### Model 1: Reduced Features (Standard Linear Regression)\n",
                "Updated to include relevant advanced features if applicable. \n",
                "Let's select: `min_nights_std`, `availability_365_std`, `review_density_std`, `poly_min_nights_sq`, `interact_min_nights_reviews_month` + Room Types."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ValueError",
                    "evalue": "'min_nights_std' is not in list",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[5], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m rt_features \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feature_names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrivate room\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntire home\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShared room\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m     11\u001b[0m reduced_features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rt_features\n\u001b[1;32m---> 13\u001b[0m reduced_indices \u001b[38;5;241m=\u001b[39m [feature_names\u001b[38;5;241m.\u001b[39mindex(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m reduced_features]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Prepare Reduced X (Add Intercept for LR)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_train_red \u001b[38;5;241m=\u001b[39m add_intercept(X_train_full[:, reduced_indices])\n",
                        "Cell \u001b[1;32mIn[5], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     10\u001b[0m rt_features \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m feature_names \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrivate room\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEntire home\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mShared room\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m f]\n\u001b[0;32m     11\u001b[0m reduced_features \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rt_features\n\u001b[1;32m---> 13\u001b[0m reduced_indices \u001b[38;5;241m=\u001b[39m [\u001b[43mfeature_names\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m reduced_features]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Prepare Reduced X (Add Intercept for LR)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m X_train_red \u001b[38;5;241m=\u001b[39m add_intercept(X_train_full[:, reduced_indices])\n",
                        "\u001b[1;31mValueError\u001b[0m: 'min_nights_std' is not in list"
                    ]
                }
            ],
            "source": [
                "# Define Reduced Feature Set (Manual Selection based on intuition/correlation)\n",
                "reduced_features = [\n",
                "    'min_nights_std', \n",
                "    'availability_365_std', \n",
                "    'review_density_std',\n",
                "    'poly_min_nights_sq',             # New Polynomial\n",
                "    'interact_min_nights_reviews_month' # New Interaction\n",
                "]\n",
                "# Add Room Type Features dynamically\n",
                "rt_features = [f for f in feature_names if 'Private room' in f or 'Entire home' in f or 'Shared room' in f]\n",
                "reduced_features += rt_features\n",
                "\n",
                "reduced_indices = [feature_names.index(f) for f in reduced_features]\n",
                "\n",
                "# Prepare Reduced X (Add Intercept for LR)\n",
                "X_train_red = add_intercept(X_train_full[:, reduced_indices])\n",
                "X_test_red = add_intercept(X_test_full[:, reduced_indices])\n",
                "\n",
                "mse_cv_red, r2_cv_red = k_fold_cross_validation(X_train_red, Y_train, model_type='linear', k=5)\n",
                "print(f\"Model 1 (Reduced) - 5-Fold CV: MSE = {mse_cv_red:.4f}, R2 = {r2_cv_red:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model 2: Full Features (Ridge Regression)\n",
                "Using all 300+ features (including OHE neighbourhoods and new interactions)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting 5-Fold CV for ridge...\n",
                        "Model 2 (Full Ridge) - 5-Fold CV: MSE = 0.1119, R2 = 0.3577\n"
                    ]
                }
            ],
            "source": [
                "X_train_ridge = add_intercept(X_train_full)\n",
                "X_test_ridge = add_intercept(X_test_full)\n",
                "\n",
                "lambda_val = 0.001\n",
                "mse_cv_full, r2_cv_full = k_fold_cross_validation(X_train_ridge, Y_train, model_type='ridge', k=5, lambda_reg=lambda_val)\n",
                "print(f\"Model 2 (Full Ridge) - 5-Fold CV: MSE = {mse_cv_full:.4f}, R2 = {r2_cv_full:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model 3: KNN Regression (Full Features)\n",
                "Distance-based model. \n",
                "**Note**: KNN is computationally expensive (O(N*M)) for prediction. We use a subset of training data or smaller K-Folds if it's too slow, but for this assignment we run full on Full Features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Starting 5-Fold CV for knn...\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# No intercept needed for KNN\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note: Running KNN CV on Full Features might be slow depending on dataset size (~30k rows).\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# We will proceed with k=5 neighbors.\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mse_cv_knn, r2_cv_knn \u001b[38;5;241m=\u001b[39m \u001b[43mk_fold_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel 3 (KNN)        - 5-Fold CV: MSE = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmse_cv_knn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R2 = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2_cv_knn\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
                        "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(X, Y, model_type, k, **kwargs)\u001b[0m\n\u001b[0;32m     27\u001b[0m     knn \u001b[38;5;241m=\u001b[39m KNNRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     28\u001b[0m     knn\u001b[38;5;241m.\u001b[39mfit(X_train_fold, Y_train_fold)\n\u001b[1;32m---> 29\u001b[0m     Y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mknn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m mse_val \u001b[38;5;241m=\u001b[39m mse(Y_val_fold, Y_pred)\n\u001b[0;32m     32\u001b[0m r2_val \u001b[38;5;241m=\u001b[39m r2_score(Y_val_fold, Y_pred)\n",
                        "Cell \u001b[1;32mIn[3], line 48\u001b[0m, in \u001b[0;36mKNNRegressor.predict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m     44\u001b[0m test_point \u001b[38;5;241m=\u001b[39m X_test[i]\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Euclidean Distance: sqrt(sum((x - y)^2))\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Broadcasting test_point across all X_train\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_point\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Find K nearest indices\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# argpartition is faster than sort for finding top k\u001b[39;00m\n\u001b[0;32m     52\u001b[0m nearest_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(distances, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk)[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk]\n",
                        "File \u001b[1;32mc:\\Users\\KUONG\\anaconda3\\envs\\lab02-airbnb-env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:2389\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2390\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[0;32m   2392\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[1;32mc:\\Users\\KUONG\\anaconda3\\envs\\lab02-airbnb-env\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# No intercept needed for KNN\n",
                "# Note: Running KNN CV on Full Features might be slow depending on dataset size (~30k rows).\n",
                "# We will proceed with k=5 neighbors.\n",
                "\n",
                "mse_cv_knn, r2_cv_knn = k_fold_cross_validation(X_train_full, Y_train, model_type='knn', k=5)\n",
                "print(f\"Model 3 (KNN)        - 5-Fold CV: MSE = {mse_cv_knn:.4f}, R2 = {r2_cv_knn:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Evaluation on Test Set\n",
                "Train strictly on Train Set, Evaluate on Test Set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "---- Final Test Set Evaluation ----\n",
                        "Model 1 (Reduced)    : MSE = 0.5771, R2 = 0.4283\n",
                        "Model 2 (Full Ridge) : MSE = 0.5013, R2 = 0.5035\n",
                        "Model 3 (KNN)        : MSE = 0.5332, R2 = 0.4719\n",
                        "\n",
                        "Conclusion: Stabilized Full Feature Linear model performed best.\n"
                    ]
                }
            ],
            "source": [
                "print(\"\\n---- Final Test Set Evaluation ----\")\n",
                "\n",
                "# Model 1 Final\n",
                "W_red = train_linear_regression(X_train_red, Y_train)\n",
                "Y_pred_red = predict_linear(X_test_red, W_red)\n",
                "test_mse_red = mse(Y_test, Y_pred_red)\n",
                "test_r2_red = r2_score(Y_test, Y_pred_red)\n",
                "\n",
                "# Model 2 Final\n",
                "W_ridge = train_ridge_regression(X_train_ridge, Y_train, lambda_reg=lambda_val)\n",
                "Y_pred_ridge = predict_linear(X_test_ridge, W_ridge)\n",
                "test_mse_ridge = mse(Y_test, Y_pred_ridge)\n",
                "test_r2_ridge = r2_score(Y_test, Y_pred_ridge)\n",
                "\n",
                "# Model 3 Final\n",
                "knn_final = KNNRegressor(k=5)\n",
                "knn_final.fit(X_train_full, Y_train)\n",
                "Y_pred_knn = knn_final.predict(X_test_full)\n",
                "test_mse_knn = mse(Y_test, Y_pred_knn)\n",
                "test_r2_knn = r2_score(Y_test, Y_pred_knn)\n",
                "\n",
                "print(f\"Model 1 (Reduced)    : MSE = {test_mse_red:.4f}, R2 = {test_r2_red:.4f}\")\n",
                "print(f\"Model 2 (Full Ridge) : MSE = {test_mse_ridge:.4f}, R2 = {test_r2_ridge:.4f}\")\n",
                "print(f\"Model 3 (KNN)        : MSE = {test_mse_knn:.4f}, R2 = {test_r2_knn:.4f}\")\n",
                "\n",
                "# Conclusion Logic\n",
                "best_r2 = max(test_r2_red, test_r2_ridge, test_r2_knn)\n",
                "if best_r2 == test_r2_knn:\n",
                "    print(\"\\nConclusion: Non-linear KNN model performed best, suggesting non-linear relationships dominate.\")\n",
                "elif best_r2 == test_r2_ridge:\n",
                "    print(\"\\nConclusion: Stabilized Full Feature Linear model performed best.\")\n",
                "else:\n",
                "    print(\"\\nConclusion: Reduced Feature Linear model performed best.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "lab02-airbnb-env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
