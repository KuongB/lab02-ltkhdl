{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# 03. Modeling and Evaluation\n",
                "\n",
                "## Objective\n",
                "Implement and compare two models using **NumPy exclusively**:\n",
                "1.  **Model 1 (OLS)**: Ordinary Least Squares Linear Regression.\n",
                "2.  **Model 2 (Lasso)**: Lasso Regression (L1 Regularization) using **Coordinate Descent**.\n",
                "\n",
                "**Evaluation Strategy**:\n",
                "- **Hold-out**: 20% of data reserved for final testing.\n",
                "- **Cross-Validation**: 5-Fold CV on the remaining 80% training data.\n",
                "- **Metrics**: Mean Squared Error (MSE) and $R^2$ Score."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 155,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Data Loading Complete ---\n",
                        "Data Shape: (48258,)\n",
                        "Column Names: ('latitude', 'longitude', 'price', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'price_log', 'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', 'neighbourhood_group_Staten Island', 'room_type_Private room', 'room_type_Shared room', 'review_activity', 'review_quality_score', 'is_high_value_core', 'interaction_nights_reviews')\n",
                        "\n",
                        "Feature Matrix X Shape: (48258, 17)\n",
                        "Target Vector Y Shape: (48258,)\n",
                        "Features used: ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'neighbourhood_group_Brooklyn', 'neighbourhood_group_Manhattan', 'neighbourhood_group_Queens', 'neighbourhood_group_Staten Island', 'room_type_Private room', 'room_type_Shared room', 'review_activity', 'review_quality_score', 'is_high_value_core', 'interaction_nights_reviews']\n"
                    ]
                }
            ],
            "source": [
                "import numpy as np\n",
                "import csv\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "PROCESSED_DATA_PATH = '../data/processed/airbnb_processed.csv'\n",
                "\n",
                "# Update dtype to reflect scaled float64 columns from preprocessing\n",
                "processed_dtype = np.dtype([\n",
                "    ('latitude', np.float64),\n",
                "    ('longitude', np.float64),\n",
                "    ('price', np.int32),           # Original Value\n",
                "    ('minimum_nights', np.float64),\n",
                "    ('number_of_reviews', np.float64),\n",
                "    ('reviews_per_month', np.float64),\n",
                "    ('calculated_host_listings_count', np.float64),\n",
                "    ('availability_365', np.float64),\n",
                "    ('price_log', np.float64),     # Target Variable Y\n",
                "    ('neighbourhood_group_Brooklyn', np.float64),\n",
                "    ('neighbourhood_group_Manhattan', np.float64),\n",
                "    ('neighbourhood_group_Queens', np.float64),\n",
                "    ('neighbourhood_group_Staten Island', np.float64),\n",
                "    ('room_type_Private room', np.float64),\n",
                "    ('room_type_Shared room', np.float64),\n",
                "    ('review_activity', np.float64),\n",
                "    ('review_quality_score', np.float64),\n",
                "    ('is_high_value_core', np.float64),\n",
                "    ('interaction_nights_reviews', np.float64) # New Feature\n",
                "])\n",
                "\n",
                "# --- 2. Data Loading Function (Using csv and NumPy) ---\n",
                "\n",
                "def load_processed_data(file_path, target_dtype):\n",
                "    with open(file_path, mode='r', encoding='utf-8') as f:\n",
                "        reader = csv.reader(f)\n",
                "        header = next(reader) \n",
                "        raw_data = list(reader)\n",
                "    \n",
                "    raw_array = np.array(raw_data, dtype=object)\n",
                "    \n",
                "    N = len(raw_array)\n",
                "    structured_data = np.zeros(N, dtype=target_dtype)\n",
                "    \n",
                "    for i, name in enumerate(target_dtype.names):\n",
                "        column_data = raw_array[:, i]\n",
                "        target_type = target_dtype[name].type\n",
                "        try:\n",
                "            structured_data[name] = column_data.astype(target_type)\n",
                "        except ValueError:\n",
                "            # Fallback for empty strings if any remain\n",
                "            column_data[column_data == ''] = '0'\n",
                "            structured_data[name] = column_data.astype(target_type)\n",
                "            \n",
                "    return header, structured_data\n",
                "\n",
                "header, data = load_processed_data(PROCESSED_DATA_PATH, processed_dtype)\n",
                "\n",
                "print(\"--- Data Loading Complete ---\")\n",
                "print(f\"Data Shape: {data.shape}\")\n",
                "print(f\"Column Names: {data.dtype.names}\")\n",
                "\n",
                "# --- Prepare X (Features) and Y (Target) ---\n",
                "target_col = 'price_log'\n",
                "exclude_cols = {'price', 'price_log'}\n",
                "feature_names = [name for name in data.dtype.names if name not in exclude_cols]\n",
                "\n",
                "Y_all = data[target_col]\n",
                "X_list = [data[name] for name in feature_names]\n",
                "X_all = np.column_stack(X_list)\n",
                "\n",
                "print(f\"\\nFeature Matrix X Shape: {X_all.shape}\")\n",
                "print(f\"Target Vector Y Shape: {Y_all.shape}\")\n",
                "print(f\"Features used: {feature_names}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Splitting\n",
                "We split the data into **Train (80%)** and **Test (20%)** sets using random shuffling.  \n",
                "The 20% independent test set will strictly be used ONLY for the final evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 156,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training Set: 38606 samples\n",
                        "Test Set: 9652 samples\n"
                    ]
                }
            ],
            "source": [
                "np.random.seed(42)\n",
                "indices = np.arange(len(X_all))\n",
                "np.random.shuffle(indices)\n",
                "\n",
                "test_size = 0.2\n",
                "split_idx = int(len(X_all) * (1 - test_size))\n",
                "\n",
                "train_indices = indices[:split_idx]\n",
                "test_indices = indices[split_idx:]\n",
                "\n",
                "X_train_full = X_all[train_indices]\n",
                "Y_train = Y_all[train_indices]\n",
                "\n",
                "X_test_full = X_all[test_indices]\n",
                "Y_test = Y_all[test_indices]\n",
                "\n",
                "print(f\"Training Set: {X_train_full.shape[0]} samples\")\n",
                "print(f\"Test Set: {X_test_full.shape[0]} samples\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Models Implementation: Theory & Code\n",
                "\n",
                "### Ordinary Least Squares (OLS)\n",
                "OLS seeks to minimize the **Sum of Squared Errors (SSE)**:\n",
                "$$ J(W) = ||Y - XW||^2_2 $$\n",
                "This problem has a closed-form solution known as the **Normal Equation**:\n",
                "$$ W = (X^T X)^{-1} X^T Y $$\n",
                "In NumPy, we use `np.linalg.lstsq` which uses SVD (Singular Value Decomposition) to solve this, offering better numerical stability than direct matrix inversion.\n",
                "\n",
                "### Lasso Regression (L1 Regularization)\n",
                "Lasso adds an L1 penalty term to the objective function to encourage sparsity (feature selection):\n",
                "$$ J(W) = \\frac{1}{2n} ||Y - XW||^2_2 + \\lambda ||W||_1 $$\n",
                "Because the L1 term ($|w|$) is **not differentiable** at $w=0$, subgradient methods can be unstable. The industry standard approach is **Coordinate Descent (Shooting Algorithm)**.\n",
                "\n",
                "**Coordinate Descent Logic:**\n",
                "We optimize one regression coefficient $w_j$ at a time while keeping others fixed. The updated value is found using the **Soft Thresholding** operator:\n",
                "$$ w_j = \\frac{S(\\rho_j, n\\lambda)}{z_j} $$\n",
                "Where:\n",
                "- $\\rho_j = X_j^T (Y - \\hat{Y}_{(-j)}) $ (Correlation between feature $j$ and partial residual)\n",
                "- $z_j = ||X_j||^2$\n",
                "- $S(x, \\gamma) = \\text{sign}(x) \\max(|x| - \\gamma, 0)$ is the Soft Thresholding function.\n",
                "\n",
                "This method is robust, converges faster, and correctly sets coefficients to exactly zero."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 157,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Feature Preparation ---\n",
                "def add_intercept(X):\n",
                "    intercept = np.ones((X.shape[0], 1))\n",
                "    return np.hstack((intercept, X))\n",
                "\n",
                "# --- Linear Models ---\n",
                "\n",
                "def train_linear_regression(X, Y):\n",
                "    # Uses Normal Equation via SVD (lstsq) for OLS\n",
                "    W, residuals, rank, singular_values = np.linalg.lstsq(X, Y, rcond=None)\n",
                "    return W\n",
                "\n",
                "def predict_linear(X, W):\n",
                "    return X @ W\n",
                "\n",
                "# --- Coordinate Descent for Lasso ---\n",
                "\n",
                "def soft_threshold(rho, lambd):\n",
                "    \"\"\"\n",
                "    Soft Thresholding Operator\n",
                "    S(rho, lambda) = sign(rho) * max(|rho| - lambda, 0)\n",
                "    \"\"\"\n",
                "    if rho < -lambd:\n",
                "        return rho + lambd\n",
                "    elif rho > lambd:\n",
                "        return rho - lambd\n",
                "    else:\n",
                "        return 0.0\n",
                "\n",
                "def train_lasso_regression(X, Y, lambda_reg=0.01, n_iterations=1000, tol=1e-4):\n",
                "    \"\"\"\n",
                "    Lasso Regression using Coordinate Descent.\n",
                "    Input lambda_reg corresponds to 'alpha' in sklearn.\n",
                "    Objective: 1/(2n) * ||y - Xw||^2 + lambda_reg * ||w||_1\n",
                "    \"\"\"\n",
                "    n_samples, n_features = X.shape\n",
                "    W = np.zeros(n_features)\n",
                "    \n",
                "    # Precompute norms of features (denominator z_j)\n",
                "    # z_j = sum(x_ij^2)\n",
                "    Z = np.sum(X**2, axis=0)\n",
                "    \n",
                "    # Current residuals r = y - Xw (initially y since w=0)\n",
                "    residuals = Y - X @ W\n",
                "    \n",
                "    # Sklearn scales the penalty by n_samples in the threshold check\n",
                "    # Threshold = n * lambda\n",
                "    threshold_const = n_samples * lambda_reg\n",
                "    \n",
                "    for iteration in range(n_iterations):\n",
                "        max_w_change = 0.0\n",
                "        \n",
                "        for j in range(n_features):\n",
                "            if Z[j] < 1e-15: continue # Skip zero variance features\n",
                "            \n",
                "            # 1. Partial Residual Calculation\n",
                "            # We want rho_j = X_j^T * (y - y_pred_without_j)\n",
                "            # y_pred_without_j = y_pred_current - X_j * w_j_current\n",
                "            # So residuals_without_j = residuals_current + X_j * w_j_current\n",
                "            \n",
                "            prev_w = W[j]\n",
                "            \n",
                "            # Add feature j back to residual to get partial residual\n",
                "            # partial_residual = residuals + X[:, j] * prev_w\n",
                "            # rho_j = X[:, j] @ partial_residual\n",
                "            # Optimization: rho_j = X_j @ residuals + Z[j] * prev_w\n",
                "            rho_j = X[:, j] @ residuals + Z[j] * prev_w\n",
                "            \n",
                "            # 2. Soft Thresholding Update\n",
                "            new_w = soft_threshold(rho_j, threshold_const) / Z[j]\n",
                "            W[j] = new_w\n",
                "            \n",
                "            # 3. Update Residuals based on change\n",
                "            # residuals_new = residuals_old - X_j * (new_w - prev_w)\n",
                "            w_diff = new_w - prev_w\n",
                "            if w_diff != 0:\n",
                "                residuals -= X[:, j] * w_diff\n",
                "                max_w_change = max(max_w_change, abs(w_diff))\n",
                "        \n",
                "        # Check for convergence\n",
                "        if max_w_change < tol:\n",
                "            # print(f\"Lasso converged at iteration {iteration}\")\n",
                "            break\n",
                "            \n",
                "    return W\n",
                "\n",
                "# --- Metrics ---\n",
                "def mse(y_true, y_pred):\n",
                "    return np.mean((y_true - y_pred) ** 2)\n",
                "\n",
                "def r2_score(y_true, y_pred):\n",
                "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
                "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
                "    return 1 - (ss_res / (ss_tot + 1e-8))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. K-Fold Cross Validation: Logic & Implementation\n",
                "\n",
                "**Objective**: To robustly estimate model performance and tune hyperparameters ($\\lambda$) without touching the Test Set.\n",
                "\n",
                "**Logic**:\n",
                "1.  Divide the `X_train_full` data into $k=5$ equal folds.\n",
                "2.  Iterate $i$ from $0$ to $k-1$:\n",
                "    -   **Validation Fold**: Fold $i$.\n",
                "    -   **Training Folds**: All folds except $i$.\n",
                "3.  Train model on Training Folds $\\rightarrow$ Predict on Validation Fold.\n",
                "4.  Average the MSE/R2 scores across all $k$ iterations.\n",
                "\n",
                "**NumPy Implementation Details**:\n",
                "-   We use `np.arange` to generate indices.\n",
                "-   We slice indices `indices[val_start:val_end]` for validation.\n",
                "-   We use `np.concatenate` to merge the remaining indices for training.\n",
                "-   **Note**: We manually add the intercept column inside the loop to ensure clean separation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 158,
            "metadata": {},
            "outputs": [],
            "source": [
                "def k_fold_cross_validation(X, Y, model_type='linear', k=5, **kwargs):\n",
                "    \"\"\"\n",
                "    Perform K-Fold CV\n",
                "    X should be passed WITHOUT intercept (Intercept added inside).\n",
                "    \"\"\"\n",
                "    fold_size = len(X) // k\n",
                "    indices = np.arange(len(X))\n",
                "    # Data is already shuffled at start, so sequential chunking is fine\n",
                "    \n",
                "    mse_scores = []\n",
                "    r2_scores = []\n",
                "    \n",
                "    for i in range(k):\n",
                "        val_start = i * fold_size\n",
                "        val_end = (i + 1) * fold_size\n",
                "        \n",
                "        val_idx = indices[val_start:val_end]\n",
                "        train_idx = np.concatenate([indices[:val_start], indices[val_end:]])\n",
                "        \n",
                "        X_train_fold, Y_train_fold = X[train_idx], Y[train_idx]\n",
                "        X_val_fold, Y_val_fold = X[val_idx], Y[val_idx]\n",
                "        \n",
                "        # Linear models need intercept added explicitly here\n",
                "        X_train_int = add_intercept(X_train_fold)\n",
                "        X_val_int = add_intercept(X_val_fold)\n",
                "        \n",
                "        if model_type == 'linear':\n",
                "            W = train_linear_regression(X_train_int, Y_train_fold)\n",
                "        elif model_type == 'lasso':\n",
                "            W = train_lasso_regression(X_train_int, Y_train_fold, **kwargs)\n",
                "        \n",
                "        Y_pred = predict_linear(X_val_int, W)\n",
                "        \n",
                "        mse_val = mse(Y_val_fold, Y_pred)\n",
                "        r2_val = r2_score(Y_val_fold, Y_pred)\n",
                "        \n",
                "        mse_scores.append(mse_val)\n",
                "        r2_scores.append(r2_val)\n",
                "        \n",
                "    return np.mean(mse_scores), np.mean(r2_scores)\n",
                "\n",
                "\n",
                "def grid_search(X, Y, model_type, param_grid_name, param_values):\n",
                "    best_score = float('inf')\n",
                "    best_param = None\n",
                "    \n",
                "    print(f\"Searching {model_type} with {param_grid_name}: {param_values}\")\n",
                "    \n",
                "    for val in param_values:\n",
                "        kwargs = {param_grid_name: val}\n",
                "        \n",
                "        if model_type == 'lasso':\n",
                "            # Coordinate Descent converges fast, can set reasonable max_iter\n",
                "            kwargs['n_iterations'] = 1000\n",
                "\n",
                "        avg_mse, avg_r2 = k_fold_cross_validation(X, Y, model_type, k=3, **kwargs)\n",
                "        \n",
                "        print(f\"  Param {val}: MSE={avg_mse:.4f}, R2={avg_r2:.4f}\")\n",
                "        \n",
                "        if avg_mse < best_score:\n",
                "            best_score = avg_mse\n",
                "            best_param = val\n",
                "            \n",
                "    print(f\"Best {param_grid_name}: {best_param}\")\n",
                "    return best_param"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hyperparameter Tuning (Lasso)\n",
                "We obtain the optimal regularization strength ($\\lambda$) for Lasso using the Grid Search defined above. OLS does not have hyperparameters."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 159,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Searching lasso with lambda_reg: [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
                        "  Param 0.0001: MSE=0.1920, R2=0.5282\n",
                        "  Param 0.001: MSE=0.1922, R2=0.5276\n",
                        "  Param 0.01: MSE=0.2007, R2=0.5067\n",
                        "  Param 0.1: MSE=0.3013, R2=0.2597\n",
                        "  Param 1.0: MSE=0.4059, R2=0.0026\n",
                        "Best lambda_reg: 0.0001\n"
                    ]
                }
            ],
            "source": [
                "# Tune Lasso\n",
                "lasso_lambdas = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
                "best_lambda_lasso = grid_search(X_train_full, Y_train, 'lasso', 'lambda_reg', lasso_lambdas)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Final Evaluation\n",
                "We now train the final models on the **Entire Training Set (80%)** using the best found parameters (for Lasso). We then evaluate them on the **Independent Test Set (20%)**.\n",
                "\n",
                "We report:\n",
                "1.  **MSE (Log Scale)**: The primary metric (loss function).\n",
                "2.  **R2 Score**: How much variance is explained.\n",
                "3.  **RMSE (Original Scale)**: For business interpretability ($ USD)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "FINAL TEST SET RESULTS\n",
                        "=======================================================\n",
                        "Model           MSE (Log)       R2 Score  \n",
                        "----------------------------------------\n",
                        "OLS (Linear)    0.1927          0.5319    \n",
                        "Lasso (l=0.0001) 0.1988          0.5172    \n"
                    ]
                }
            ],
            "source": [
                "# Train Final Models on Full Training Set\n",
                "X_train_int = add_intercept(X_train_full)\n",
                "X_test_int = add_intercept(X_test_full)\n",
                "\n",
                "# 1. OLS\n",
                "W_ols = train_linear_regression(X_train_int, Y_train)\n",
                "Y_pred_ols = predict_linear(X_test_int, W_ols)\n",
                "\n",
                "# 2. Lasso (Best)\n",
                "# Using Coordinate Descent impl\n",
                "W_lasso = train_lasso_regression(X_train_int, Y_train, lambda_reg=0.0001, n_iterations=1000)\n",
                "Y_pred_lasso = predict_linear(X_test_int, W_lasso)\n",
                "\n",
                "# Metrics\n",
                "mse_ols = mse(Y_test, Y_pred_ols)\n",
                "mse_lasso = mse(Y_test, Y_pred_lasso)\n",
                "\n",
                "r2_ols = r2_score(Y_test, Y_pred_ols)\n",
                "r2_lasso = r2_score(Y_test, Y_pred_lasso)\n",
                "\n",
                "print(\"\\nFINAL TEST SET RESULTS\")\n",
                "print(\"=======================================================\")\n",
                "print(\"{:<15} {:<15} {:<10}\".format(\"Model\", \"MSE (Log)\", \"R2 Score\"))\n",
                "print(\"-\" * 40)\n",
                "print(\"{:<15} {:<15.4f} {:<10.4f}\".format(\"OLS (Linear)\", mse_ols, r2_ols))\n",
                "print(\"{:<15} {:<15.4f} {:<10.4f}\".format(f\"Lasso (l={best_lambda_lasso})\", mse_lasso, r2_lasso))\n",
                "\n",
                "# RMSE Original Scale Calculation (USD)\n",
                "# Best model select\n",
                "models = {'OLS': mse_ols, 'Lasso': mse_lasso}\n",
                "best_model_name = min(models, key=models.get)\n",
                "\n",
                "if best_model_name == 'OLS': best_pred = Y_pred_ols\n",
                "elif best_model_name == 'Lasso': best_pred = Y_pred_lasso\n",
                "\n",
                "# Transform back: exp(log_price) - 1\n",
                "Y_test_orig = np.exp(Y_test) - 1\n",
                "Y_pred_orig = np.exp(best_pred) - 1\n",
                "\n",
                "rmse_price = np.sqrt(mse(Y_test_orig, Y_pred_orig))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verification: Comparison with Scikit-learn\n",
                "\n",
                "In this final section, we verify the correctness of our custom **NumPy** implementations by comparing them against the industry-standard **Scikit-learn** versions. \n",
                "\n",
                "- We rely on `sklearn.linear_model` for the models.\n",
                "- We continue to use our **custom NumPy metrics** (`mse`, `r2_score`) for evaluation to ensure an apples-to-apples comparison."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 161,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Scikit-learn Verification ---\n",
                        "\n",
                        "=================================================================================\n",
                        "FINAL COMPARISON: CUSTOM NUMPY vs SCIKIT-LEARN\n",
                        "=================================================================================\n",
                        "Model      Implementation  MSE (Log)       R2         RMSE (Orig $)  \n",
                        "---------------------------------------------------------------------------\n",
                        "OLS        Custom NumPy    0.1927          0.5319     $80.38          \n",
                        "OLS        Scikit-learn    0.1927          0.5319     $80.38          \n",
                        "---------------------------------------------------------------------------\n",
                        "Lasso      Custom NumPy    0.1988          0.5172     $81.47          \n",
                        "Lasso      Scikit-learn    0.1928          0.5319     $80.43          \n",
                        "=================================================================================\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.linear_model import LinearRegression, Lasso\n",
                "\n",
                "print(\"--- Scikit-learn Verification ---\\n\")\n",
                "\n",
                "# 1. Scikit-learn OLS\n",
                "sklearn_ols = LinearRegression()\n",
                "sklearn_ols.fit(X_train_full, Y_train)\n",
                "Y_pred_sklearn_ols = sklearn_ols.predict(X_test_full)\n",
                "\n",
                "mse_sklearn_ols = mse(Y_test, Y_pred_sklearn_ols)\n",
                "r2_sklearn_ols = r2_score(Y_test, Y_pred_sklearn_ols)\n",
                "rmse_sklearn_ols = np.sqrt(mse(np.exp(Y_test)-1, np.exp(Y_pred_sklearn_ols)-1))\n",
                "\n",
                "# 2. Scikit-learn Lasso (Using same best_lambda_lasso)\n",
                "# Note: Sklearn alpha is our lambda. Sklearn optimizes 1/(2n) * ||y - Xw||^2 + alpha * ||w||_1\n",
                "# Our objective function was the same, so parameter should match directly.\n",
                "sklearn_lasso = Lasso(alpha=best_lambda_lasso)\n",
                "sklearn_lasso.fit(X_train_full, Y_train)\n",
                "Y_pred_sklearn_lasso = sklearn_lasso.predict(X_test_full)\n",
                "\n",
                "mse_sklearn_lasso = mse(Y_test, Y_pred_sklearn_lasso)\n",
                "r2_sklearn_lasso = r2_score(Y_test, Y_pred_sklearn_lasso)\n",
                "rmse_sklearn_lasso = np.sqrt(mse(np.exp(Y_test)-1, np.exp(Y_pred_sklearn_lasso)-1))\n",
                "\n",
                "# Calculate RMSE for Custom models for the table\n",
                "rmse_ols = np.sqrt(mse(np.exp(Y_test)-1, np.exp(Y_pred_ols)-1))\n",
                "rmse_lasso = np.sqrt(mse(np.exp(Y_test)-1, np.exp(Y_pred_lasso)-1))\n",
                "\n",
                "print(\"=================================================================================\")\n",
                "print(\"FINAL COMPARISON: CUSTOM NUMPY vs SCIKIT-LEARN\")\n",
                "print(\"=================================================================================\")\n",
                "print(\"{:<10} {:<15} {:<15} {:<10} {:<15}\".format(\"Model\", \"Implementation\", \"MSE (Log)\", \"R2\", \"RMSE (Orig $)\"))\n",
                "print(\"-\" * 75)\n",
                "\n",
                "print(\"{:<10} {:<15} {:<15.4f} {:<10.4f} ${:<15.2f}\".format(\"OLS\", \"Custom NumPy\", mse_ols, r2_ols, rmse_ols))\n",
                "print(\"{:<10} {:<15} {:<15.4f} {:<10.4f} ${:<15.2f}\".format(\"OLS\", \"Scikit-learn\", mse_sklearn_ols, r2_sklearn_ols, rmse_sklearn_ols))\n",
                "print(\"-\" * 75)\n",
                "print(\"{:<10} {:<15} {:<15.4f} {:<10.4f} ${:<15.2f}\".format(\"Lasso\", \"Custom NumPy\", mse_lasso, r2_lasso, rmse_lasso))\n",
                "print(\"{:<10} {:<15} {:<15.4f} {:<10.4f} ${:<15.2f}\".format(\"Lasso\", \"Scikit-learn\", mse_sklearn_lasso, r2_sklearn_lasso, rmse_sklearn_lasso))\n",
                "print(\"=================================================================================\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "python3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
